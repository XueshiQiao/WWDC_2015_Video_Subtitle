
00:00:24.516 --> 00:00:27.036 A:middle
[Applause]

00:00:27.536 --> 00:00:31.316 A:middle
&gt;&gt; ANTHONY CHIVETTA:
Good morning and welcome

00:00:31.316 --> 00:00:34.046 A:middle
to building responsive and
efficient apps with GCD.

00:00:34.306 --> 00:00:37.276 A:middle
We're so excited to see so
many of you here interested

00:00:37.276 --> 00:00:40.416 A:middle
in learning about how Grand
Central Dispatch can help you

00:00:40.886 --> 00:00:43.256 A:middle
adapt your application
to all of our platforms.

00:00:43.646 --> 00:00:46.826 A:middle
I'm Anthony and my teammate
Daniel will be presenting this

00:00:46.826 --> 00:00:47.336 A:middle
talk with me.

00:00:48.526 --> 00:00:52.176 A:middle
Grand Central Dispatch or GCD is
a technology that was introduced

00:00:52.176 --> 00:00:53.336 A:middle
with OS X Snow Leopard.

00:00:53.696 --> 00:00:57.626 A:middle
At that time our brand new
Mac was the MacBook Pro

00:00:57.896 --> 00:00:59.286 A:middle
with the Core II Duo.

00:00:59.656 --> 00:01:02.646 A:middle
One of the selling points
of GCD at the time was



00:00:59.656 --> 00:01:02.646 A:middle
One of the selling points
of GCD at the time was

00:01:02.646 --> 00:01:05.126 A:middle
that it would allow you to
take advantage of both cores,

00:01:05.126 --> 00:01:07.236 A:middle
running different parts of
your application concurrently,

00:01:07.536 --> 00:01:09.866 A:middle
and make threading
really, really easy.

00:01:10.356 --> 00:01:13.746 A:middle
We think that use of GCD has
really stood the test of time.

00:01:14.656 --> 00:01:19.036 A:middle
Today our top-of-the-line Mac
Pro has many, many more Cores

00:01:19.606 --> 00:01:22.446 A:middle
and GCD is still a great
way to take advantage of all

00:01:22.446 --> 00:01:23.626 A:middle
of those computing resources.

00:01:24.346 --> 00:01:28.056 A:middle
But just as GCD is a great
way to use all the resources

00:01:28.056 --> 00:01:30.886 A:middle
on the high end it can
help your application adapt

00:01:30.976 --> 00:01:32.596 A:middle
to smaller environments.

00:01:34.176 --> 00:01:36.136 A:middle
For example, the new MacBook

00:01:36.136 --> 00:01:38.686 A:middle
that we recently released is
the first paneless design.

00:01:39.316 --> 00:01:44.846 A:middle
While this is an advantage in
terms of size of the machine,

00:01:44.846 --> 00:01:47.336 A:middle
it also presents unique
challenges in terms

00:01:47.336 --> 00:01:49.366 A:middle
of how we manage the
thermal properties.

00:01:49.446 --> 00:01:52.986 A:middle
I'll talk a little bit later
about how your app can use GCD

00:01:53.176 --> 00:01:54.946 A:middle
to run more efficiently
in this environment.

00:01:55.696 --> 00:01:59.536 A:middle
We also have iOS 9 with
new multitasking features.

00:01:59.946 --> 00:02:02.836 A:middle
This is the first time your
app had to run side-by-side,



00:01:59.946 --> 00:02:02.836 A:middle
This is the first time your
app had to run side-by-side,

00:02:02.976 --> 00:02:06.896 A:middle
quite literally, with other
applications on the system.

00:02:06.896 --> 00:02:09.836 A:middle
GCD can inform the system
what kind of work you're doing

00:02:09.985 --> 00:02:13.406 A:middle
and better share
resources between your app

00:02:13.476 --> 00:02:15.046 A:middle
and the other apps
present on the screen.

00:02:15.536 --> 00:02:21.656 A:middle
Of course, Watch OS brings your
code to our smallest platform.

00:02:22.516 --> 00:02:26.346 A:middle
GCD is a way that you can help
the system know which parts

00:02:26.346 --> 00:02:28.916 A:middle
of your code you should
run in order to be able

00:02:28.916 --> 00:02:34.056 A:middle
to have a responsive application
on a device this size.

00:02:38.976 --> 00:02:41.176 A:middle
So a brief outline of what we
are going to go through today.

00:02:41.686 --> 00:02:42.556 A:middle
I'm going to start

00:02:42.556 --> 00:02:45.066 A:middle
by introducing something called
Quality of Service classes.

00:02:45.646 --> 00:02:49.496 A:middle
This is an API that we released
with iOS 8 and OS X Yosemite.

00:02:50.826 --> 00:02:53.276 A:middle
Daniel's going to come up and
go through some design patterns

00:02:53.276 --> 00:02:54.706 A:middle
for using GCD, and how

00:02:54.706 --> 00:02:56.576 A:middle
to integrate QLS
with these patterns.

00:02:57.676 --> 00:03:00.296 A:middle
I'll then go through some
details about threads, queues,



00:02:57.676 --> 00:03:00.296 A:middle
I'll then go through some
details about threads, queues,

00:03:00.336 --> 00:03:03.126 A:middle
and run loops that can
make using GCD easier.

00:03:03.706 --> 00:03:06.686 A:middle
And finally, we'll conclude
with a brief section on how

00:03:06.686 --> 00:03:08.816 A:middle
to understand crash reports
when you're using GCD.

00:03:09.596 --> 00:03:13.486 A:middle
But first a little
bit of background.

00:03:14.336 --> 00:03:17.386 A:middle
So we have your awesome app.

00:03:17.386 --> 00:03:20.266 A:middle
And you start executing the
app, the user taps the icon,

00:03:20.266 --> 00:03:21.206 A:middle
loads it from a finder.

00:03:21.976 --> 00:03:24.156 A:middle
We will begin executing
your code in main

00:03:24.576 --> 00:03:26.366 A:middle
and you'll have your
initial main thread

00:03:26.366 --> 00:03:27.336 A:middle
that every app starts with.

00:03:28.326 --> 00:03:31.666 A:middle
You call UI application
main, or NSApplication main,

00:03:31.666 --> 00:03:34.026 A:middle
and that's going to bring
up a run loop on the thread

00:03:34.146 --> 00:03:35.216 A:middle
and the Framework code.

00:03:35.776 --> 00:03:39.686 A:middle
And then that thread is going
to sit there waiting for events.

00:03:40.306 --> 00:03:42.776 A:middle
At some point something
is going to happen.

00:03:42.776 --> 00:03:44.206 A:middle
Maybe you'll get a
delegate method call

00:03:44.206 --> 00:03:45.846 A:middle
out to your UI application
delegate.

00:03:45.846 --> 00:03:48.556 A:middle
At this point, your code
begins to run and needs

00:03:48.556 --> 00:03:51.206 A:middle
to do something; let's say it
wants to read from a database.

00:03:52.146 --> 00:03:54.296 A:middle
You go out and, you'll
access that file on disk.

00:03:55.256 --> 00:03:56.656 A:middle
That data will come back.

00:03:58.366 --> 00:03:59.976 A:middle
You'll update the
user interface.



00:04:02.436 --> 00:04:05.666 A:middle
And then finally return
control back to the Frameworks

00:04:05.666 --> 00:04:07.246 A:middle
and continue waiting for
events on the thread.

00:04:08.356 --> 00:04:10.196 A:middle
This works great until that read

00:04:10.196 --> 00:04:13.626 A:middle
from the database takes
a little bit of time.

00:04:13.766 --> 00:04:18.446 A:middle
At that point on OS X you might
see a spinning wait cursor.

00:04:18.846 --> 00:04:21.676 A:middle
On iOS the app would hang and
might even get terminated.

00:04:22.416 --> 00:04:25.806 A:middle
This is a poor and
unresponsive user experience.

00:04:26.266 --> 00:04:30.106 A:middle
And this is where
GCD can come in

00:04:30.106 --> 00:04:32.296 A:middle
and help make things
a little bit easier.

00:04:32.296 --> 00:04:33.836 A:middle
You'll get your delegate
method of call out.

00:04:33.836 --> 00:04:38.946 A:middle
But instead of doing the work
immediately you can create a GCD

00:04:39.616 --> 00:04:42.576 A:middle
queue, use dispatch async to
move the work on to the cue.

00:04:43.636 --> 00:04:45.756 A:middle
Your code executes
asynchronously

00:04:45.756 --> 00:04:47.016 A:middle
with respect to the main thread.

00:04:48.166 --> 00:04:50.246 A:middle
When you have the
data available,

00:04:50.246 --> 00:04:53.346 A:middle
you can dispatch async back to
the main thread, update the UI.

00:04:53.346 --> 00:04:57.356 A:middle
Now, the advantage here is that
while your work is happening

00:04:57.356 --> 00:05:00.596 A:middle
on that GCD queue, the main
thread can continue waiting



00:04:57.356 --> 00:05:00.596 A:middle
on that GCD queue, the main
thread can continue waiting

00:05:00.596 --> 00:05:01.256 A:middle
for events.

00:05:01.436 --> 00:05:02.536 A:middle
It stays responsive.

00:05:02.576 --> 00:05:04.546 A:middle
The user continues to
get a great experience,

00:05:05.346 --> 00:05:08.056 A:middle
and everyone is happy.

00:05:08.296 --> 00:05:09.436 A:middle
Now, I'm hoping this
is a pattern

00:05:09.436 --> 00:05:10.486 A:middle
that is familiar to all of you.

00:05:10.486 --> 00:05:12.266 A:middle
We are not going to
go through the details

00:05:12.266 --> 00:05:13.526 A:middle
of how to accomplish this.

00:05:14.016 --> 00:05:16.576 A:middle
If this is not familiar, I
highly encourage you to head

00:05:16.576 --> 00:05:19.906 A:middle
up to Presidio after this
and check out the talk there

00:05:19.906 --> 00:05:23.056 A:middle
which will go through
the pattern in detail.

00:05:23.766 --> 00:05:25.846 A:middle
One thing you might have
not thought about before,

00:05:25.846 --> 00:05:28.936 A:middle
is we now have two threads
that both want to execute code.

00:05:29.346 --> 00:05:31.016 A:middle
Your main thread wants
to handle new events

00:05:31.146 --> 00:05:32.536 A:middle
and the GCD queue wants

00:05:32.536 --> 00:05:34.286 A:middle
to execute the block
you dispatched to it.

00:05:34.286 --> 00:05:37.376 A:middle
And maybe we're only on
a single core device.

00:05:38.046 --> 00:05:40.366 A:middle
In this case, which
thread do we execute?

00:05:45.286 --> 00:05:47.676 A:middle
This is where Quality of
Service classes come into play.

00:05:48.436 --> 00:05:53.156 A:middle
As I mentioned this is a new
API in iOS 8 and 10 Yosemite

00:05:53.156 --> 00:05:54.546 A:middle
that we released last year.

00:05:55.306 --> 00:05:58.806 A:middle
We have four Quality of Service
classes: user interactive,

00:05:59.036 --> 00:06:01.616 A:middle
user initiated, utility
and background.



00:05:59.036 --> 00:06:01.616 A:middle
user initiated, utility
and background.

00:06:02.806 --> 00:06:04.776 A:middle
These are ways you tell
the system what kind

00:06:04.776 --> 00:06:07.896 A:middle
of work you're doing, and in
turn, it allows the system

00:06:07.896 --> 00:06:10.246 A:middle
to provide a variety
of resource controls

00:06:10.716 --> 00:06:12.526 A:middle
to most effectively
execute your code.

00:06:12.526 --> 00:06:16.376 A:middle
When I say resource controls,
what am I talking about?

00:06:16.946 --> 00:06:19.946 A:middle
Our system has support for
CPU scheduling priority,

00:06:20.426 --> 00:06:22.286 A:middle
which threads do we
run, in what order?

00:06:22.286 --> 00:06:26.146 A:middle
I/O priority, how do we
execute I/O with deference

00:06:26.146 --> 00:06:27.326 A:middle
to other I/O in the system.

00:06:28.216 --> 00:06:30.666 A:middle
Timer coalescing, which
is a power-saving feature.

00:06:31.316 --> 00:06:33.836 A:middle
And whether we run
the CPU in through-put

00:06:33.836 --> 00:06:35.356 A:middle
or in efficiency-oriented mode.

00:06:35.566 --> 00:06:37.426 A:middle
Do we want to get the most
performance, or do we want

00:06:37.426 --> 00:06:41.656 A:middle
to execute the code in the
most energy-efficient manner?

00:06:41.806 --> 00:06:45.086 A:middle
In an ideal world we tune each
of these configuration values

00:06:45.386 --> 00:06:48.306 A:middle
for each platform or device and
piece of code that's running,

00:06:49.006 --> 00:06:51.166 A:middle
but obviously this would
get out of hand quickly.

00:06:51.456 --> 00:06:54.526 A:middle
There's a lot of values hard
to tune correctly and a lot

00:06:54.526 --> 00:06:58.176 A:middle
of platforms where
your code can run.

00:06:58.176 --> 00:07:00.306 A:middle
Quality of Service
Classes are designed



00:06:58.176 --> 00:07:00.306 A:middle
Quality of Service
Classes are designed

00:07:00.306 --> 00:07:03.206 A:middle
to be a single abstract
parameter that you can use

00:07:03.236 --> 00:07:06.336 A:middle
to communicate the intent and
classification of your work.

00:07:06.336 --> 00:07:08.566 A:middle
Rather than trying to tune all

00:07:08.566 --> 00:07:11.566 A:middle
of these specific configuration
values you simply say

00:07:11.716 --> 00:07:13.096 A:middle
"I'm doing user initiated work"

00:07:13.526 --> 00:07:15.676 A:middle
and the system will
automatically pick the right

00:07:15.676 --> 00:07:17.556 A:middle
values for that platform
and device.

00:07:18.096 --> 00:07:22.776 A:middle
So I mentioned we have four
Quality of Service classes.

00:07:22.856 --> 00:07:25.276 A:middle
Let me talk through them briefly
and what they are used for.

00:07:26.126 --> 00:07:27.466 A:middle
The first is user interactive.

00:07:28.086 --> 00:07:29.576 A:middle
This is the main thread.

00:07:30.146 --> 00:07:33.426 A:middle
Imagine you have an iOS app,
the user has their finger

00:07:33.426 --> 00:07:34.506 A:middle
on the screen and is dragging.

00:07:35.086 --> 00:07:37.946 A:middle
The main thread needs to
be responsive in order

00:07:37.946 --> 00:07:39.776 A:middle
to deliver the next
frame of animation

00:07:39.776 --> 00:07:40.736 A:middle
as the user is dragging.

00:07:41.166 --> 00:07:45.506 A:middle
The main user interactive code
is specifically the code needed

00:07:45.856 --> 00:07:48.366 A:middle
in order to keep that 60
frames per second animation

00:07:48.366 --> 00:07:49.156 A:middle
running smoothly.

00:07:51.326 --> 00:07:55.096 A:middle
So you want to ask yourself:
Is this work actively involved

00:07:55.096 --> 00:07:55.886 A:middle
in updating the UI

00:07:56.436 --> 00:07:57.846 A:middle
when considering
whether something should

00:07:57.846 --> 00:07:58.926 A:middle
be user-interactive.

00:07:59.826 --> 00:08:04.386 A:middle
This isn't loading the content
potentially of that scroll view.



00:07:59.826 --> 00:08:04.386 A:middle
This isn't loading the content
potentially of that scroll view.

00:08:04.976 --> 00:08:07.926 A:middle
It is just drawing
the new animation.

00:08:08.576 --> 00:08:11.866 A:middle
We talk about user initiated
as being loading the results

00:08:11.866 --> 00:08:13.856 A:middle
of an action done by the user.

00:08:14.316 --> 00:08:15.726 A:middle
So this might be
as I'm scrolling

00:08:15.726 --> 00:08:18.546 A:middle
through scroll view loading
the data for the next cells

00:08:19.156 --> 00:08:22.596 A:middle
or if I'm in a photo or mail
application and tap an email

00:08:22.596 --> 00:08:26.176 A:middle
or photo, loading the
full size photo or e-mail,

00:08:26.666 --> 00:08:29.226 A:middle
those kinds of actions are what
we talk about as user initiated.

00:08:30.236 --> 00:08:33.206 A:middle
The question is, is
this work required

00:08:33.206 --> 00:08:34.716 A:middle
to continue user interaction?

00:08:35.096 --> 00:08:36.775 A:middle
It's helpful not to any about it

00:08:36.775 --> 00:08:38.726 A:middle
as user initiated
but user blocking.

00:08:38.726 --> 00:08:41.816 A:middle
If the user can't continue
to make meaningful progress

00:08:41.816 --> 00:08:44.596 A:middle
with your application, user
initiated is the correct class.

00:08:46.236 --> 00:08:48.866 A:middle
Utility is for those things
that the user may have started,

00:08:49.596 --> 00:08:50.896 A:middle
or may have started
automatically,

00:08:51.156 --> 00:08:54.176 A:middle
but are longer running tasks
that don't prevent the user

00:08:54.176 --> 00:08:55.536 A:middle
from continuing to use your app.

00:08:57.176 --> 00:08:59.126 A:middle
You want to ask yourself,
is the user aware

00:08:59.126 --> 00:09:01.356 A:middle
of the progress of this work?



00:08:59.126 --> 00:09:01.356 A:middle
of the progress of this work?

00:09:01.356 --> 00:09:03.896 A:middle
If you were a magazine app
downloading a new issue,

00:09:04.336 --> 00:09:06.586 A:middle
is something that
the user can continue

00:09:06.586 --> 00:09:07.616 A:middle
to use the app while
it's happening.

00:09:07.616 --> 00:09:09.976 A:middle
They can read old
issues or browse around.

00:09:10.496 --> 00:09:12.136 A:middle
You may have a progress bar

00:09:12.416 --> 00:09:13.986 A:middle
and the user is aware
of the progress.

00:09:14.506 --> 00:09:16.746 A:middle
This is a great thing
to classify as utility.

00:09:18.836 --> 00:09:21.256 A:middle
Finally, background is
for everything else.

00:09:21.466 --> 00:09:23.216 A:middle
The user is not actively
watching.

00:09:23.216 --> 00:09:25.216 A:middle
Any kind of maintenance
task, cleanup work,

00:09:26.776 --> 00:09:29.486 A:middle
database vacuuming
would all be background.

00:09:30.316 --> 00:09:32.776 A:middle
And the question is basically,

00:09:32.816 --> 00:09:34.286 A:middle
is the user unaware
of this work?

00:09:34.956 --> 00:09:37.256 A:middle
Now background work is
interesting because you want

00:09:37.256 --> 00:09:39.436 A:middle
to think about when
you're doing it.

00:09:39.436 --> 00:09:41.746 A:middle
I encourage you to take a look

00:09:41.746 --> 00:09:44.666 A:middle
at the writing energy efficient
code sessions from last year

00:09:44.776 --> 00:09:47.406 A:middle
that talk about how to do
background work effectively

00:09:47.516 --> 00:09:55.276 A:middle
if your app has significant
work in this class.

00:09:56.666 --> 00:09:57.816 A:middle
So I mentioned our new MacBook.

00:09:58.836 --> 00:10:02.546 A:middle
As I said, this is
the first fanless Mac.



00:09:58.836 --> 00:10:02.546 A:middle
As I said, this is
the first fanless Mac.

00:10:02.546 --> 00:10:05.676 A:middle
In previous MacBooks
that had a fan,

00:10:05.676 --> 00:10:08.896 A:middle
as the machine is doing more and
more work and generating more

00:10:08.896 --> 00:10:11.766 A:middle
and more energy and
therefore heat we can bring

00:10:11.766 --> 00:10:14.436 A:middle
up the fan speed to help
dissipate the heat faster.

00:10:15.256 --> 00:10:17.836 A:middle
The new MacBook is
incredibly energy efficient.

00:10:18.246 --> 00:10:22.446 A:middle
We don't need a fan to dissipate
heat in most circumstances,

00:10:23.426 --> 00:10:26.896 A:middle
but running this machine at
full speed is still going

00:10:26.896 --> 00:10:29.346 A:middle
to generate some energy
we need to dissipate.

00:10:30.266 --> 00:10:32.426 A:middle
But our ability to dissipate
heat is at a constant rate.

00:10:33.356 --> 00:10:37.806 A:middle
We have other techniques to make
sure we can keep the enclosure

00:10:37.806 --> 00:10:40.006 A:middle
of the machine at an appropriate
temperature for the user.

00:10:40.846 --> 00:10:44.226 A:middle
Imagine you have an app using
and you have work happening

00:10:44.226 --> 00:10:46.026 A:middle
at all four Quality
of Service classes.

00:10:46.856 --> 00:10:49.886 A:middle
You are driving the machine
hard, using a lot of energy

00:10:50.176 --> 00:10:52.906 A:middle
and we need to help control
that amount of energy in order

00:10:52.906 --> 00:10:54.976 A:middle
to keep the machine at a
reasonable temperature.

00:10:55.616 --> 00:10:58.986 A:middle
Well, what we can do is we can
start to squeeze the amount

00:10:58.986 --> 00:11:00.126 A:middle
of work we are going to do



00:10:58.986 --> 00:11:00.126 A:middle
of work we are going to do

00:11:00.476 --> 00:11:02.646 A:middle
at the less important
Quality of Service classes.

00:11:03.136 --> 00:11:06.586 A:middle
This allows us to manage
the system's use of energy,

00:11:07.896 --> 00:11:10.406 A:middle
keep the machine
responsive, and make sure

00:11:10.406 --> 00:11:13.726 A:middle
that there's no responsiveness
issues visible to the user.

00:11:14.666 --> 00:11:16.096 A:middle
This way it's very important

00:11:16.096 --> 00:11:17.806 A:middle
that you have your work
correctly classified

00:11:17.866 --> 00:11:21.516 A:middle
when running on a
machine like this.

00:11:21.926 --> 00:11:24.846 A:middle
I also mentioned iOS and the
new multitasking features.

00:11:25.556 --> 00:11:28.436 A:middle
You can imagine in the old
world we might have your app

00:11:28.436 --> 00:11:31.776 A:middle
with its main thread and maybe
it has additional dispatch

00:11:31.816 --> 00:11:35.346 A:middle
thread running but now
I bring up another app.

00:11:35.836 --> 00:11:40.066 A:middle
And that app also is going
to have a main thread.

00:11:41.246 --> 00:11:42.916 A:middle
And then I also have
a Picture in Picture.

00:11:43.646 --> 00:11:45.206 A:middle
That has a thread
decoding the video.

00:11:46.026 --> 00:11:48.966 A:middle
Okay, but I've only
got two CPUs.

00:11:49.896 --> 00:11:52.256 A:middle
So if I have to use one
to decode the video,

00:11:52.536 --> 00:11:53.576 A:middle
what do I do with the next one?

00:11:54.286 --> 00:11:55.676 A:middle
This is another place
for Quality

00:11:55.676 --> 00:11:57.356 A:middle
of Service classes can
really help you out.

00:11:57.856 --> 00:12:01.716 A:middle
Indicating to the operating
system the Quality of Service



00:11:57.856 --> 00:12:01.716 A:middle
Indicating to the operating
system the Quality of Service

00:12:01.716 --> 00:12:04.026 A:middle
of each thread we
can correctly decide

00:12:04.026 --> 00:12:05.966 A:middle
where to marshall those
available resources.

00:12:07.426 --> 00:12:10.196 A:middle
So with that I'll hand things
off to Daniel who will go

00:12:10.196 --> 00:12:13.096 A:middle
through specific design patterns
and how to apply Quality

00:12:13.096 --> 00:12:13.976 A:middle
of Service to those patterns.

00:12:14.516 --> 00:12:20.806 A:middle
[Applause]

00:12:21.306 --> 00:12:21.796 A:middle
&gt;&gt; DANIEL STEFFEN: Good morning.

00:12:21.796 --> 00:12:22.396 A:middle
Thanks, Anthony.

00:12:23.136 --> 00:12:25.016 A:middle
So in this section
we are going to look

00:12:25.016 --> 00:12:29.606 A:middle
at a couple specific examples
GCD design and how QoS applies.

00:12:30.606 --> 00:12:34.236 A:middle
The fundamentals we are
going to look at for GCD

00:12:34.236 --> 00:12:38.326 A:middle
and QoS are how QoS
can be specified

00:12:38.396 --> 00:12:41.866 A:middle
at the individual block level
as well as on QoS has a whole

00:12:42.446 --> 00:12:44.446 A:middle
and how dispatch async

00:12:44.446 --> 00:12:47.746 A:middle
and related APIs
propagate QoS automatically

00:12:47.746 --> 00:12:49.056 A:middle
from the submitting thread

00:12:49.096 --> 00:12:50.836 A:middle
to the asynchronously
running block,

00:12:51.516 --> 00:12:54.656 A:middle
and then how the system can
resolve some priority inversions

00:12:54.656 --> 00:12:55.766 A:middle
for you automatically.

00:12:56.196 --> 00:13:00.236 A:middle
We won't have time to go into
depth on the specific API calls.



00:12:56.196 --> 00:13:00.236 A:middle
We won't have time to go into
depth on the specific API calls.

00:13:00.796 --> 00:13:05.526 A:middle
If you want more details
I encourage you to look

00:13:05.526 --> 00:13:07.536 A:middle
at the session from last
year, "Power, performance

00:13:07.536 --> 00:13:09.456 A:middle
and diagnostics: What's
New in GCD and XPC",

00:13:09.456 --> 00:13:13.356 A:middle
that you can see this on the
developer website where we went

00:13:13.356 --> 00:13:17.286 A:middle
into much more specific
detail on how to use the APIs

00:13:17.286 --> 00:13:18.676 A:middle
that were new last year.

00:13:19.876 --> 00:13:22.956 A:middle
So first example is the
example that Anthony had earlier

00:13:23.206 --> 00:13:25.376 A:middle
where we performed
some asynchronous work

00:13:26.156 --> 00:13:30.866 A:middle
and do some I/O on the GCD
cue off of the main thread.

00:13:32.266 --> 00:13:37.316 A:middle
How does this example
fit in with QoS,

00:13:37.316 --> 00:13:39.046 A:middle
what are the appropriate Quality

00:13:39.046 --> 00:13:40.476 A:middle
of Service classes
to apply here?

00:13:40.476 --> 00:13:44.606 A:middle
On the left-hand side we have
the main thread, of course.

00:13:44.606 --> 00:13:47.466 A:middle
As Anthony mentioned, this is
where the UI rendering happens,

00:13:47.516 --> 00:13:48.846 A:middle
this is where the
event handling happens,

00:13:49.386 --> 00:13:51.876 A:middle
the appropriate caller service
call here is user interactive.

00:13:52.566 --> 00:13:54.106 A:middle
Nothing you have
to do to get this.

00:13:54.216 --> 00:13:55.986 A:middle
The main thread of
the application comes

00:13:55.986 --> 00:13:57.266 A:middle
up at this Quality
of Service class.

00:13:57.266 --> 00:14:00.786 A:middle
On the right-hand
side of the screen,



00:13:57.266 --> 00:14:00.786 A:middle
On the right-hand
side of the screen,

00:14:00.786 --> 00:14:03.236 A:middle
that's the asynchronous work not
happening on the main thread.

00:14:03.766 --> 00:14:06.866 A:middle
Obviously, we shouldn't be
running user interactive.

00:14:07.136 --> 00:14:08.766 A:middle
We're not doing the
UI rendering here.

00:14:09.186 --> 00:14:12.216 A:middle
But, say the user tapped on the
document icon and is waiting

00:14:12.486 --> 00:14:16.196 A:middle
for his document to open.

00:14:16.196 --> 00:14:18.776 A:middle
The user is blocked in
his progress with the app.

00:14:18.906 --> 00:14:21.866 A:middle
He can still interact with the
UI but can't do what he wants,

00:14:21.866 --> 00:14:24.856 A:middle
which is edit the document
or view the document.

00:14:24.856 --> 00:14:26.756 A:middle
User initiated is the
appropriate Quality

00:14:26.756 --> 00:14:27.556 A:middle
of Service Class here.

00:14:30.286 --> 00:14:32.916 A:middle
So how do we achieve
that with the GCD API.

00:14:33.596 --> 00:14:37.696 A:middle
It turns out you don't
have to do anything,

00:14:37.696 --> 00:14:39.226 A:middle
it will work automatically.

00:14:39.616 --> 00:14:41.646 A:middle
But it's important to
understand why that is.

00:14:41.646 --> 00:14:44.916 A:middle
Let's look at that in detail.

00:14:44.916 --> 00:14:47.386 A:middle
Everything starts with
this initial dispatch async

00:14:47.456 --> 00:14:50.036 A:middle
that works off the asynchronous
work from the main thread.

00:14:50.756 --> 00:14:53.596 A:middle
As I mentioned in
the previous slide,

00:14:53.676 --> 00:14:56.036 A:middle
dispatch async does automatic
propagation of Quality

00:14:56.036 --> 00:14:59.206 A:middle
of Service from the
submitting thread to the queue

00:14:59.646 --> 00:15:02.696 A:middle
where you submit the
block to execute.



00:14:59.646 --> 00:15:02.696 A:middle
where you submit the
block to execute.

00:15:02.696 --> 00:15:07.226 A:middle
Now, in this case there's
a special rule that applies

00:15:07.226 --> 00:15:09.426 A:middle
which is that we
automatically translate Quality

00:15:09.426 --> 00:15:12.756 A:middle
of Service Class user
interactive to user initiated.

00:15:13.286 --> 00:15:16.086 A:middle
We do this so we don't
accidentaly over-propagate the

00:15:16.086 --> 00:15:18.216 A:middle
Quality of Service Class
that should be restricted

00:15:18.216 --> 00:15:19.976 A:middle
to the main thread
and UI rendering.

00:15:19.976 --> 00:15:22.286 A:middle
We can take advantage
of that here,

00:15:22.436 --> 00:15:23.796 A:middle
because that is exactly
what we want.

00:15:24.346 --> 00:15:25.966 A:middle
That is typically the case

00:15:26.536 --> 00:15:30.036 A:middle
by having the automatic
propagation run the block

00:15:30.456 --> 00:15:33.956 A:middle
on the queue at Quality of
Service Class user initiated.

00:15:34.726 --> 00:15:36.996 A:middle
Of course when you go back to
the main thread to update the UI

00:15:36.996 --> 00:15:40.276 A:middle
with the results this automatic
propagation will also try

00:15:40.686 --> 00:15:41.446 A:middle
to take place.

00:15:42.086 --> 00:15:45.666 A:middle
Because the user main
thread is Quality

00:15:45.666 --> 00:15:47.066 A:middle
of Service Class
user interactive,

00:15:47.576 --> 00:15:48.576 A:middle
that will take priority.

00:15:48.716 --> 00:15:51.286 A:middle
It will not lower if
you go to a thread

00:15:51.506 --> 00:15:52.866 A:middle
that has some assigned Quality

00:15:52.866 --> 00:15:54.246 A:middle
of Service Class
like the main thread.

00:15:54.866 --> 00:15:56.896 A:middle
Here we will ignore
this propagated value

00:15:57.336 --> 00:16:01.386 A:middle
and run the UI update block
at the bottom at Quality



00:15:57.336 --> 00:16:01.386 A:middle
and run the UI update block
at the bottom at Quality

00:16:01.386 --> 00:16:02.636 A:middle
of Service Class
user interactive.

00:16:03.286 --> 00:16:08.756 A:middle
So we call this QoS propagation
property inferred QoS.

00:16:09.516 --> 00:16:13.216 A:middle
And this is QoS captured at the
time the block gets submitted

00:16:13.216 --> 00:16:15.626 A:middle
to a queue and with
the special rule

00:16:15.626 --> 00:16:17.206 A:middle
that we translate
user interactive

00:16:17.296 --> 00:16:19.076 A:middle
to user initiated as mentioned.

00:16:20.406 --> 00:16:22.706 A:middle
This propagated Quality
of Service is used

00:16:22.786 --> 00:16:24.826 A:middle
if the destination where
the block is submitted

00:16:24.826 --> 00:16:27.156 A:middle
to does not have its own
quality of service specified

00:16:28.286 --> 00:16:31.566 A:middle
and does not lower QoS if
you go to the main thread

00:16:31.566 --> 00:16:34.626 A:middle
that has its own high
Quality of Service assigned.

00:16:36.756 --> 00:16:42.636 A:middle
So next example is something
that doesn't open automatically,

00:16:42.636 --> 00:16:44.666 A:middle
long running job, say a
long running calculation

00:16:45.096 --> 00:16:48.486 A:middle
that dispatch asyncs it off
the main thread so as not

00:16:48.486 --> 00:16:53.136 A:middle
to interfere with the UI and it
operates on a queue concurrently

00:16:53.136 --> 00:16:58.906 A:middle
with the main thread and maybe
updates the UI with the progress

00:16:58.996 --> 00:17:00.446 A:middle
by asyncing back a block



00:16:58.996 --> 00:17:00.446 A:middle
by asyncing back a block

00:17:00.546 --> 00:17:02.596 A:middle
that updates some
progress UI element.

00:17:05.816 --> 00:17:08.356 A:middle
What are the appropriate
Quality of Service classes here?

00:17:08.715 --> 00:17:11.425 A:middle
On the left-hand side you
have user interactive,

00:17:11.915 --> 00:17:14.965 A:middle
and the right-hand side as
Anthony has described earlier,

00:17:14.965 --> 00:17:17.026 A:middle
this is something where Quality

00:17:17.026 --> 00:17:18.925 A:middle
of Service utility
is appropriate.

00:17:19.336 --> 00:17:20.506 A:middle
It's something long running.

00:17:20.816 --> 00:17:22.205 A:middle
The user can continue
to use the UI.

00:17:22.496 --> 00:17:24.376 A:middle
He's not waiting for
the results immediately

00:17:24.376 --> 00:17:28.036 A:middle
and he can see the progress,
and he probably initiated it

00:17:28.036 --> 00:17:32.106 A:middle
in some fashion but it is not
blocking his immediate progress.

00:17:33.696 --> 00:17:37.746 A:middle
So how do we make this
happen with the GCD API?

00:17:38.256 --> 00:17:40.476 A:middle
The simplest solution
is to focus on the point

00:17:40.476 --> 00:17:42.526 A:middle
where the work is
generated, initiated.

00:17:43.266 --> 00:17:44.876 A:middle
This is initial dispatch async.

00:17:45.586 --> 00:17:47.986 A:middle
We can tag the block
that we submit

00:17:48.676 --> 00:17:50.606 A:middle
with the appropriate
Quality of Service Class

00:17:50.606 --> 00:17:53.656 A:middle
by using the dispatch block
create with QoS class API,

00:17:54.456 --> 00:17:57.376 A:middle
we pass in the block that
we want to execute as well

00:17:57.486 --> 00:17:58.586 A:middle
as the Quality of Service Class

00:17:58.666 --> 00:18:00.346 A:middle
that we would like,
utility here.



00:17:58.666 --> 00:18:00.346 A:middle
that we would like,
utility here.

00:18:00.766 --> 00:18:02.616 A:middle
The resulting block
object is what we pass

00:18:02.686 --> 00:18:06.656 A:middle
through dispatch async and when
that gets executed that will run

00:18:06.836 --> 00:18:08.366 A:middle
at Quality of Service
Class utility

00:18:09.226 --> 00:18:12.756 A:middle
and by finding this
initial generation point

00:18:12.756 --> 00:18:14.526 A:middle
where the Quality of
Service Class changes,

00:18:15.076 --> 00:18:17.716 A:middle
the automatic propagation
further downstream,

00:18:17.946 --> 00:18:21.326 A:middle
if any from that block, will
then be able to take advantage

00:18:21.326 --> 00:18:22.566 A:middle
of the automatic propagation.

00:18:22.926 --> 00:18:25.396 A:middle
So that anything that gets
generated asynchronously

00:18:25.396 --> 00:18:27.526 A:middle
from that work will
happen automatically

00:18:27.526 --> 00:18:30.346 A:middle
at the correct Quality
of Service class,

00:18:30.346 --> 00:18:32.616 A:middle
continue to be a utility without
you having to do anything.

00:18:33.246 --> 00:18:39.476 A:middle
So this block QoS is
created like we saw

00:18:39.476 --> 00:18:42.426 A:middle
in the previous slide by adding
an explicit QoS attribute

00:18:42.426 --> 00:18:45.306 A:middle
to the wrapped up block object
to the block you provide

00:18:45.806 --> 00:18:49.176 A:middle
at the appropriate time to use
this is at the point when work

00:18:49.176 --> 00:18:50.536 A:middle
of a different class
is generated.

00:18:51.396 --> 00:18:54.146 A:middle
Another use case for
QoS in a block object is

00:18:54.656 --> 00:18:58.716 A:middle
if you have needs to capture
the Quality of Service class

00:18:58.716 --> 00:19:00.406 A:middle
in a block that you
are provided,



00:18:58.716 --> 00:19:00.406 A:middle
in a block that you
are provided,

00:19:00.856 --> 00:19:05.106 A:middle
say in a callback block scenario
where you are writing an API

00:19:05.296 --> 00:19:08.036 A:middle
where somebody provides you with
a callback block and you want

00:19:08.036 --> 00:19:11.016 A:middle
to store that block and submit
it later from a different thread

00:19:11.396 --> 00:19:13.366 A:middle
or queue, but you really want

00:19:13.366 --> 00:19:16.106 A:middle
to get this propagation right
the same way dispatch async

00:19:16.106 --> 00:19:17.786 A:middle
propagates the Quality
of Service.

00:19:18.276 --> 00:19:21.476 A:middle
You can do this by using the
dispatch block assign current

00:19:22.106 --> 00:19:24.136 A:middle
flag, pass that through
dispatch block create,

00:19:24.136 --> 00:19:27.126 A:middle
and that will capture the
current QoS and execution state,

00:19:27.516 --> 00:19:30.576 A:middle
store it in a rapid block and
when you submit that block later

00:19:30.576 --> 00:19:33.496 A:middle
on to a queue it will run
with that assigned value.

00:19:34.756 --> 00:19:39.896 A:middle
To look at another example,
we have an application

00:19:40.346 --> 00:19:42.036 A:middle
that performs a UI action.

00:19:42.036 --> 00:19:43.056 A:middle
And during the performance

00:19:43.056 --> 00:19:45.336 A:middle
of that action it notices
some maintenance condition,

00:19:45.336 --> 00:19:46.876 A:middle
some cleanup condition
that should happen.

00:19:47.316 --> 00:19:49.566 A:middle
Say you have a database, it
has too many loose objects,

00:19:49.566 --> 00:19:53.036 A:middle
has to do some compaction,
some clean up task,

00:19:53.936 --> 00:20:00.996 A:middle
typical example again
of the use of GCD



00:19:53.936 --> 00:20:00.996 A:middle
typical example again
of the use of GCD

00:20:01.246 --> 00:20:04.106 A:middle
where you would execute
dispatch async to run

00:20:04.106 --> 00:20:07.306 A:middle
that maintenance task
on a background queue

00:20:07.306 --> 00:20:09.676 A:middle
and the left-hand
side, of course,

00:20:10.086 --> 00:20:13.606 A:middle
being the user initiated
-- user interactive again.

00:20:13.606 --> 00:20:16.836 A:middle
The appropriate Quality of
Service class here would be

00:20:17.166 --> 00:20:19.076 A:middle
to use Quality of
Service background.

00:20:19.806 --> 00:20:22.056 A:middle
Currently given by the
title of the slide,

00:20:22.256 --> 00:20:24.326 A:middle
this is a maintenance
operation that is unrelated

00:20:24.326 --> 00:20:25.466 A:middle
to what the user is doing.

00:20:25.466 --> 00:20:27.986 A:middle
You notice this condition
while you were going along

00:20:28.586 --> 00:20:31.296 A:middle
and the user is unaware
that this is occurring.

00:20:31.446 --> 00:20:34.036 A:middle
You are kind of doing
work, the app does work

00:20:34.036 --> 00:20:35.166 A:middle
on its own behalf here.

00:20:35.486 --> 00:20:39.556 A:middle
How do we achieve running
Quality of Service background?

00:20:39.956 --> 00:20:43.426 A:middle
One thing we can do is to use
the block API we saw earlier

00:20:43.426 --> 00:20:47.666 A:middle
and have the initial async
to this queue be the Quality

00:20:47.846 --> 00:20:49.176 A:middle
of Service background.

00:20:49.616 --> 00:20:52.206 A:middle
Maybe this is a case where there
is multiple places in the app

00:20:52.266 --> 00:20:53.876 A:middle
that generate this
type of clean up work

00:20:53.876 --> 00:20:57.196 A:middle
and there's multiple ways
that you need to operate

00:20:57.196 --> 00:20:58.526 A:middle
on the database in this mode.

00:20:58.526 --> 00:21:02.366 A:middle
It might be appropriate to have
a queue specifically dedicated



00:20:58.526 --> 00:21:02.366 A:middle
It might be appropriate to have
a queue specifically dedicated

00:21:02.406 --> 00:21:05.046 A:middle
to this task so you
can also create queues

00:21:05.226 --> 00:21:07.786 A:middle
with assigned Quality
of Service.

00:21:07.786 --> 00:21:11.386 A:middle
You use dispatch queue
attr make with QoS class,

00:21:11.386 --> 00:21:13.296 A:middle
passing in background
in this example.

00:21:13.756 --> 00:21:16.286 A:middle
The resulting attribute can be
passed to dispatch queue create

00:21:16.796 --> 00:21:19.136 A:middle
to create a clean up queue
in this example here.

00:21:20.556 --> 00:21:23.046 A:middle
By having Quality of Service
class assigned to the queue,

00:21:23.616 --> 00:21:26.496 A:middle
you get the automatic
propagation for dispatch async,

00:21:27.706 --> 00:21:31.576 A:middle
again user initiated
from the main thread we

00:21:31.976 --> 00:21:34.876 A:middle
in fact ignore this propagated
value because you are submitting

00:21:34.876 --> 00:21:36.826 A:middle
to a queue that has its
own assigned Quality

00:21:36.826 --> 00:21:39.516 A:middle
of Service Class and use
what the queue says instead.

00:21:40.006 --> 00:21:43.026 A:middle
So the block that you submitted
will run at background instead

00:21:43.366 --> 00:21:44.466 A:middle
of what it would have otherwise

00:21:44.696 --> 00:21:46.466 A:middle
if it had the automatic
propagation.

00:21:47.636 --> 00:21:50.936 A:middle
For a case like this where there
is a maintenance task unrelated

00:21:50.936 --> 00:21:54.736 A:middle
to the execution flow,
it may be appropriate

00:21:55.316 --> 00:21:59.936 A:middle
to consider whether the dispatch
block detached flag is of use.



00:22:00.626 --> 00:22:04.476 A:middle
This is a way to tell the
operating system that the work

00:22:04.476 --> 00:22:06.266 A:middle
that you are doing in this
block has nothing to do

00:22:06.266 --> 00:22:07.706 A:middle
with the flow of execution.

00:22:08.106 --> 00:22:11.526 A:middle
And it will in particular
opt out of propagation of QoS

00:22:11.666 --> 00:22:15.196 A:middle
but also opt out of capturing
things like the activity ID

00:22:15.196 --> 00:22:18.346 A:middle
if you are using that for
the activity tracing feature

00:22:18.346 --> 00:22:20.606 A:middle
that we introduced at
the conference last year.

00:22:22.146 --> 00:22:25.566 A:middle
And other properties of
the execution context.

00:22:26.436 --> 00:22:30.436 A:middle
Now, of course, even if you have
work that should always kind

00:22:30.436 --> 00:22:32.676 A:middle
of be at Quality of Service
background that is clean

00:22:32.676 --> 00:22:34.206 A:middle
up work, there may
be exceptions.

00:22:34.676 --> 00:22:38.896 A:middle
Maybe there is some kind of log
out feature where the user logs

00:22:38.896 --> 00:22:41.506 A:middle
out of his account and you
have to delete the database

00:22:42.066 --> 00:22:44.056 A:middle
and delete the user's
private data.

00:22:44.486 --> 00:22:46.816 A:middle
That's something that the
user want to see complete.

00:22:46.866 --> 00:22:48.496 A:middle
This is not a background task.

00:22:49.066 --> 00:22:54.616 A:middle
You may have a need to override
or opt out of this queue

00:22:54.616 --> 00:22:56.996 A:middle
or this runs at background
property

00:22:56.996 --> 00:22:58.196 A:middle
that you have set up here.

00:22:58.976 --> 00:23:01.646 A:middle
If you just use the automatic
propagation feature here,



00:22:58.976 --> 00:23:01.646 A:middle
If you just use the automatic
propagation feature here,

00:23:01.646 --> 00:23:05.416 A:middle
as before we would ignore
the user initiated Quality

00:23:05.506 --> 00:23:07.336 A:middle
of Service except
here, of course,

00:23:07.336 --> 00:23:09.376 A:middle
that's the right Quality
of Service to use.

00:23:09.666 --> 00:23:10.896 A:middle
That's what we really
want to happen.

00:23:10.896 --> 00:23:12.626 A:middle
The user is waiting for
this log out to complete.

00:23:13.316 --> 00:23:14.526 A:middle
How to achieve that?

00:23:14.526 --> 00:23:19.276 A:middle
Use the dispatch block
enforce QoS class flag,

00:23:19.396 --> 00:23:21.776 A:middle
block create along with the
block you want to execute.

00:23:22.156 --> 00:23:23.506 A:middle
That indicates to the system

00:23:23.816 --> 00:23:26.856 A:middle
that you really want the value
that's in the block as opposed

00:23:26.856 --> 00:23:27.986 A:middle
to the one that's in the queue.

00:23:27.986 --> 00:23:30.896 A:middle
So you can override the
queue's value display.

00:23:32.196 --> 00:23:36.986 A:middle
If you do that, the block
will get executed at Quality

00:23:36.986 --> 00:23:39.236 A:middle
of Service class user
initiated in this example.

00:23:40.616 --> 00:23:42.086 A:middle
But of course, here

00:23:42.086 --> 00:23:44.116 A:middle
in the picture you can
see now we have a case

00:23:44.116 --> 00:23:47.106 A:middle
where you have several queue
with potentially two blocks

00:23:47.106 --> 00:23:49.486 A:middle
in queue at the same time at
different priority levels.

00:23:50.256 --> 00:23:53.156 A:middle
That's the case of
asynchronous priority inversion.

00:23:53.546 --> 00:23:56.196 A:middle
A High QoS block may be
submitted to a serial queue,

00:23:56.196 --> 00:23:58.236 A:middle
but there is already
work in queue or running

00:23:58.606 --> 00:24:00.026 A:middle
at a lower Quality of Service



00:23:58.606 --> 00:24:00.026 A:middle
at a lower Quality of Service

00:24:00.276 --> 00:24:01.966 A:middle
and you have a priority
inversion.

00:24:03.166 --> 00:24:05.436 A:middle
GCD helps you with that
if you use a serial queue

00:24:05.726 --> 00:24:08.936 A:middle
by raising the work that
is already there, running

00:24:08.936 --> 00:24:11.776 A:middle
or enqueued until you reach the
high Quality of Service block.

00:24:12.986 --> 00:24:15.226 A:middle
This is something that
happens behind the scenes

00:24:15.356 --> 00:24:17.036 A:middle
with QoS override.

00:24:17.036 --> 00:24:20.306 A:middle
It is not something that the
over-written blocks can see

00:24:20.306 --> 00:24:22.896 A:middle
themselves or if they
propagate work further along

00:24:23.286 --> 00:24:26.406 A:middle
asynchronously they will
propagate at the original QoS

00:24:26.406 --> 00:24:29.766 A:middle
that they were as, but they
will actually be running

00:24:29.766 --> 00:24:32.496 A:middle
at a higher priority to
resolve the inversion.

00:24:34.316 --> 00:24:38.616 A:middle
So to recap, queue QoS is
mostly appropriate for queues

00:24:38.666 --> 00:24:42.746 A:middle
that are single purpose in the
app or take inputs from all

00:24:42.746 --> 00:24:44.876 A:middle
over the place where you
don't want the priority

00:24:44.876 --> 00:24:46.926 A:middle
of the submitted to be
important, but the priority

00:24:47.316 --> 00:24:50.516 A:middle
of that purpose and it
may also be appropriate

00:24:50.896 --> 00:24:54.966 A:middle
to use the detached block API
for such types of workload,

00:24:54.966 --> 00:24:56.966 A:middle
especially if they are
maintenance or background.

00:24:58.946 --> 00:25:03.586 A:middle
And using QoS on the queue
causes us to ignore the QoS



00:24:58.946 --> 00:25:03.586 A:middle
And using QoS on the queue
causes us to ignore the QoS

00:25:03.586 --> 00:25:06.856 A:middle
in the asynchronous
blocks exempt in the case

00:25:06.856 --> 00:25:08.756 A:middle
where you also use
that enforce flag.

00:25:09.616 --> 00:25:15.976 A:middle
Last example is use of
serial queues as locks.

00:25:16.536 --> 00:25:18.236 A:middle
This is a very common use of GCD

00:25:18.296 --> 00:25:21.716 A:middle
where you have some shared
data structure in the app

00:25:22.106 --> 00:25:24.656 A:middle
where you want locked access
to that data structure

00:25:25.176 --> 00:25:29.936 A:middle
and you can use GCD by
creating a serial queue

00:25:30.046 --> 00:25:33.926 A:middle
with a dispatch queue serial
flag at the data structure

00:25:33.986 --> 00:25:38.956 A:middle
and then use dispatch sync to
execute a critical section block

00:25:39.406 --> 00:25:42.266 A:middle
on that queue where that
block has exclusive access

00:25:42.266 --> 00:25:43.006 A:middle
to the data structure.

00:25:43.826 --> 00:25:47.576 A:middle
How does QoS fit into this?

00:25:47.576 --> 00:25:49.896 A:middle
It is important to note
that dispatch sync,

00:25:50.136 --> 00:25:56.136 A:middle
it executes the block when that
lock is obtained on the thread

00:25:56.136 --> 00:25:59.396 A:middle
that calls dispatch sync
and releases the thread

00:25:59.396 --> 00:26:01.606 A:middle
with the block returns.



00:25:59.396 --> 00:26:01.606 A:middle
with the block returns.

00:26:01.776 --> 00:26:03.846 A:middle
In this case we don't need any
additional [inaudible] threads,

00:26:03.846 --> 00:26:07.906 A:middle
we have a thread called dispatch
sync and we will execute

00:26:07.906 --> 00:26:10.656 A:middle
that block at the Quality of
Service of the calling thread,

00:26:11.026 --> 00:26:12.166 A:middle
user interactive here.

00:26:13.426 --> 00:26:15.156 A:middle
Of course you have
synchronization

00:26:15.156 --> 00:26:16.246 A:middle
because you have other queues

00:26:16.316 --> 00:26:18.026 A:middle
or threads accessing
this data structure.

00:26:18.446 --> 00:26:19.726 A:middle
Maybe you have a Quality

00:26:19.726 --> 00:26:22.446 A:middle
of Service utility thread
also calling dispatch sync

00:26:22.446 --> 00:26:25.316 A:middle
on this queue to get
the exclusive access

00:26:25.346 --> 00:26:26.116 A:middle
to the data structure.

00:26:26.626 --> 00:26:28.906 A:middle
Again, the same thing
will happen if he comes

00:26:28.906 --> 00:26:32.496 A:middle
in later he will block waiting
to get the exclusive access.

00:26:33.016 --> 00:26:36.236 A:middle
Then execute that block on
his own thread at Quality

00:26:36.236 --> 00:26:37.206 A:middle
of Service utility

00:26:37.456 --> 00:26:40.306 A:middle
at the calling threads
Quality of Service.

00:26:41.426 --> 00:26:44.776 A:middle
Now, of course in this,
if this acquisition

00:26:44.776 --> 00:26:46.386 A:middle
of this exclusive
access happened

00:26:46.386 --> 00:26:48.646 A:middle
in the other order we
would again have a case

00:26:48.646 --> 00:26:50.106 A:middle
of priority inversion.

00:26:50.106 --> 00:26:52.956 A:middle
If the utility guy comes in
first and takes the lock,

00:26:52.956 --> 00:26:55.606 A:middle
you have the main thread
waiting on a utility thread.

00:26:56.456 --> 00:26:58.516 A:middle
And that's obviously
undesirable.

00:26:59.086 --> 00:27:02.126 A:middle
So Quality of Service
inheritance,



00:26:59.086 --> 00:27:02.126 A:middle
So Quality of Service
inheritance,

00:27:02.316 --> 00:27:04.346 A:middle
synchronous priority inversion
will help you with that.

00:27:04.766 --> 00:27:06.356 A:middle
A high priority service
thread waiting

00:27:06.356 --> 00:27:09.246 A:middle
for the lower course
work, we'll resolve

00:27:09.316 --> 00:27:12.536 A:middle
that by raising the Quality
of Service of waited on work

00:27:13.166 --> 00:27:16.046 A:middle
for the duration of the
waiter and this happens

00:27:16.046 --> 00:27:18.716 A:middle
if you use a serial queue
with the dispatch sync

00:27:18.716 --> 00:27:20.266 A:middle
or the dispatch block wait APIs.

00:27:20.916 --> 00:27:22.906 A:middle
It also happens if you
use pthread mutex lock

00:27:23.106 --> 00:27:26.426 A:middle
or any APIs built on top
of it such as NSLock.

00:27:26.916 --> 00:27:32.346 A:middle
It is important to note, there
are APIs that do not do this.

00:27:32.346 --> 00:27:35.306 A:middle
Dispatch semaphores do not
admit a concept of ownership

00:27:35.306 --> 00:27:36.826 A:middle
so the system cannot determine

00:27:36.826 --> 00:27:38.986 A:middle
who will eventually
signal the semaphore.

00:27:39.086 --> 00:27:40.646 A:middle
There cannot be any resolution

00:27:40.646 --> 00:27:42.616 A:middle
of priority inversion
in that case.

00:27:42.616 --> 00:27:44.596 A:middle
If you have priority inversions

00:27:45.026 --> 00:27:47.436 A:middle
where you find high
priority waiter

00:27:47.436 --> 00:27:49.526 A:middle
in a dispatch semaphore wait,

00:27:50.086 --> 00:27:54.636 A:middle
where a low priority worker
is performing some work,

00:27:54.636 --> 00:27:57.436 A:middle
you may have to switch
to dispatch block wait,

00:27:57.436 --> 00:27:58.006 A:middle
where you can wait

00:27:58.006 --> 00:28:00.246 A:middle
on an explicit entity
that we can raise.



00:27:58.006 --> 00:28:00.246 A:middle
on an explicit entity
that we can raise.

00:28:00.246 --> 00:28:05.416 A:middle
With that I hand it back to
Anthony to talk about queues,

00:28:05.416 --> 00:28:06.076 A:middle
threads and run loops.

00:28:07.516 --> 00:28:12.326 A:middle
[Applause]

00:28:12.826 --> 00:28:13.306 A:middle
&gt;&gt; ANTHONY CHIVETTA:
Thanks, Daniel.

00:28:14.686 --> 00:28:17.506 A:middle
Hopefully that whet your
appetite for Quality of Service

00:28:17.656 --> 00:28:20.036 A:middle
and you'll go back and take
a look at the applications

00:28:20.036 --> 00:28:22.026 A:middle
and how you can adopt
Quality of Service.

00:28:22.306 --> 00:28:25.976 A:middle
I want to go through now other
topics around queues, threads

00:28:25.976 --> 00:28:30.896 A:middle
and run loops in the hopes that
it might make a greater adoption

00:28:30.896 --> 00:28:33.466 A:middle
of GCD easier for you and
help provide a little context

00:28:33.466 --> 00:28:34.796 A:middle
as you debug your application.

00:28:35.376 --> 00:28:39.956 A:middle
To remind ourselves we
have our application

00:28:40.276 --> 00:28:41.376 A:middle
with our main thread.

00:28:41.846 --> 00:28:43.746 A:middle
There's a GCD thread pool

00:28:44.126 --> 00:28:47.426 A:middle
that services all blocks
you might put on GCD queues

00:28:47.696 --> 00:28:49.666 A:middle
and you have some set of
queues in your application.

00:28:50.946 --> 00:28:53.806 A:middle
Imagine on the main thread
you execute this code.

00:28:54.106 --> 00:28:56.256 A:middle
You dispatch async
on to some queue.

00:28:57.396 --> 00:29:01.876 A:middle
A block, and we will bring
up a thread for that block.



00:28:57.396 --> 00:29:01.876 A:middle
A block, and we will bring
up a thread for that block.

00:29:02.786 --> 00:29:04.206 A:middle
Start executing your code.

00:29:04.466 --> 00:29:07.316 A:middle
We'll execute performSelector
withObject afterDelay,

00:29:07.366 --> 00:29:10.006 A:middle
and that will put a timer source

00:29:10.326 --> 00:29:11.546 A:middle
on the current thread's
run loop.

00:29:11.546 --> 00:29:16.156 A:middle
Now what do we think
happens a second later?

00:29:16.666 --> 00:29:17.736 A:middle
Well, it turns out when

00:29:17.736 --> 00:29:20.786 A:middle
that block completes the
thread might just go away.

00:29:20.966 --> 00:29:23.576 A:middle
These are threads from
our ephemeral thread pool.

00:29:23.886 --> 00:29:26.056 A:middle
They don't have any
guaranteed lifetime.

00:29:26.056 --> 00:29:27.656 A:middle
We may destroy the thread.

00:29:28.326 --> 00:29:30.116 A:middle
Of course even if
the thread stays

00:29:30.116 --> 00:29:32.976 A:middle
around no one is actually
running that run loop.

00:29:33.296 --> 00:29:35.256 A:middle
That timer is never
going to be able to fire.

00:29:36.036 --> 00:29:38.806 A:middle
This is sort of an interesting
interaction that can happen

00:29:38.806 --> 00:29:43.556 A:middle
as you mix run loop based and
dispatch queue based APIs.

00:29:43.846 --> 00:29:48.576 A:middle
So kind of -- to kind of briefly
summarize the differences

00:29:48.576 --> 00:29:52.626 A:middle
between run loops and serial
queues, run loops are bound

00:29:52.626 --> 00:29:53.516 A:middle
to a particular thread.

00:29:54.306 --> 00:29:58.036 A:middle
As an API you generally see them
get delegate method callbacks.

00:29:59.186 --> 00:30:02.506 A:middle
They have an auto release pool
that pops after each iteration



00:29:59.186 --> 00:30:02.506 A:middle
They have an auto release pool
that pops after each iteration

00:30:02.506 --> 00:30:03.746 A:middle
through all the run loop sources

00:30:04.256 --> 00:30:07.036 A:middle
and run loop can be used
reentrantly, it's possible

00:30:07.036 --> 00:30:10.026 A:middle
to respin the run loop from
the call out on that run loop.

00:30:10.026 --> 00:30:13.126 A:middle
On the other hand, serial queues

00:30:13.446 --> 00:30:17.426 A:middle
or dispatch queues use
ephemeral threads that come

00:30:17.426 --> 00:30:19.376 A:middle
from our Grand Central
Dispatch thread pool.

00:30:20.056 --> 00:30:22.316 A:middle
They generally take
blocks as their callbacks,

00:30:22.976 --> 00:30:25.466 A:middle
or APIs that use them generally
take blocks as callbacks.

00:30:26.646 --> 00:30:29.146 A:middle
The autorelease pool on a
serial queue will only pop

00:30:29.186 --> 00:30:30.976 A:middle
when a thread goes
completely idle.

00:30:31.716 --> 00:30:34.846 A:middle
This could never happen if your
application is constantly busy.

00:30:35.606 --> 00:30:38.996 A:middle
So it's important to not rely on
the autorelease pool that comes

00:30:38.996 --> 00:30:40.816 A:middle
for free when you use dispatch.

00:30:41.206 --> 00:30:43.296 A:middle
If you are going to be auto
releasing a lot of objects,

00:30:43.296 --> 00:30:45.406 A:middle
make sure you have your
auto release pools in place.

00:30:45.856 --> 00:30:49.636 A:middle
Finally, serial queues
are not a reentrant

00:30:50.086 --> 00:30:51.946 A:middle
or recursive locking structure.

00:30:52.326 --> 00:30:55.236 A:middle
You want to make sure as you're
designing your application's use

00:30:55.236 --> 00:30:57.226 A:middle
of queues, you don't run
into a case where you need

00:30:57.226 --> 00:30:58.316 A:middle
to use them reentrantly.

00:30:59.256 --> 00:31:01.966 A:middle
These rules are bound
together in the sense



00:30:59.256 --> 00:31:01.966 A:middle
These rules are bound
together in the sense

00:31:01.966 --> 00:31:05.166 A:middle
that main thread's run loop is
also exposed as the main queue.

00:31:05.626 --> 00:31:09.276 A:middle
It is easy with respect to
the main thread to jump back

00:31:09.276 --> 00:31:10.596 A:middle
and forth between these worlds.

00:31:11.166 --> 00:31:13.986 A:middle
So if you go back to
the timer example,

00:31:13.986 --> 00:31:16.216 A:middle
we kind of compare the APIs.

00:31:16.326 --> 00:31:17.416 A:middle
For run loops we have things

00:31:17.416 --> 00:31:20.556 A:middle
like NSObjects performSelector
withObject afterDelay.

00:31:21.266 --> 00:31:23.586 A:middle
Or NSTimer
scheduledTimerWithTimeInterval

00:31:23.586 --> 00:31:25.966 A:middle
that installs a timer
on the current run loop.

00:31:26.756 --> 00:31:29.116 A:middle
In the dispatch world, we have
things like dispatch after

00:31:29.626 --> 00:31:32.536 A:middle
and dispatch sources with
dispatch source set timer,

00:31:32.536 --> 00:31:36.006 A:middle
which can create a
timer that will fire

00:31:36.006 --> 00:31:40.006 A:middle
by putting a block on a queue.

00:31:40.516 --> 00:31:44.426 A:middle
Now, I mentioned that Grand
Central Dispatch uses ephemeral

00:31:44.496 --> 00:31:47.606 A:middle
threads, now let me talk
you through how that works.

00:31:48.436 --> 00:31:51.886 A:middle
Imagine I'm executing and I do a
whole bunch of dispatch asyncs.

00:31:53.336 --> 00:31:55.746 A:middle
I put those on the
global queues.

00:31:57.056 --> 00:32:00.446 A:middle
The system is going to take
a thread from the thread pool



00:31:57.056 --> 00:32:00.446 A:middle
The system is going to take
a thread from the thread pool

00:32:00.446 --> 00:32:01.726 A:middle
and give it to the first block.

00:32:02.316 --> 00:32:03.356 A:middle
Send it running on its way.

00:32:04.106 --> 00:32:05.876 A:middle
Take another thread, give
it to the second block

00:32:05.876 --> 00:32:07.356 A:middle
and send it running on its way.

00:32:07.896 --> 00:32:09.576 A:middle
Say we're a two core device,

00:32:10.006 --> 00:32:11.916 A:middle
these threads are both
running, actively executing.

00:32:12.356 --> 00:32:13.296 A:middle
We stop here.

00:32:13.616 --> 00:32:16.726 A:middle
We have one thread per
core which is ideal.

00:32:17.786 --> 00:32:20.316 A:middle
Now, when that first block
finishes executing we'll take

00:32:20.346 --> 00:32:23.456 A:middle
the thread, give it to
the next one, and so on.

00:32:24.096 --> 00:32:25.566 A:middle
And this works really well.

00:32:25.956 --> 00:32:29.796 A:middle
Until one of our blocks
needs access to a resource

00:32:29.796 --> 00:32:30.786 A:middle
that isn't yet available.

00:32:32.346 --> 00:32:33.406 A:middle
We call this waiting.

00:32:33.746 --> 00:32:37.026 A:middle
A thread will wait
and suspend execution

00:32:37.096 --> 00:32:40.426 A:middle
when it needs a resource
such as I/O or a lock.

00:32:41.186 --> 00:32:43.336 A:middle
Now, you might also hear
this referred to as blocking.

00:32:43.756 --> 00:32:45.176 A:middle
I will call it waiting today.

00:32:45.176 --> 00:32:46.786 A:middle
So if I talk about
blocks blocking

00:32:46.786 --> 00:32:49.066 A:middle
for the next five minutes
you'll get confused

00:32:49.476 --> 00:32:52.436 A:middle
but you'll hear it called
blocking in many other contexts.

00:32:54.006 --> 00:32:56.836 A:middle
What is interesting from this
from the GCD perspective,

00:32:56.836 --> 00:33:01.606 A:middle
we want one block or one thread
executing actively per core



00:32:56.836 --> 00:33:01.606 A:middle
we want one block or one thread
executing actively per core

00:33:01.606 --> 00:33:02.206 A:middle
on the device.

00:33:03.076 --> 00:33:06.486 A:middle
So when a thread waits, we are
going to bring up another thread

00:33:06.486 --> 00:33:08.806 A:middle
in our thread pool
up until some limit

00:33:09.616 --> 00:33:11.766 A:middle
so that we can have one
thread executing per core.

00:33:13.306 --> 00:33:15.646 A:middle
So let's imagine we
have these four blocks,

00:33:15.906 --> 00:33:18.356 A:middle
we're executing the first
two on two different threads

00:33:19.156 --> 00:33:22.326 A:middle
and the first guy goes
hey, I need to do some I/O.

00:33:22.966 --> 00:33:24.876 A:middle
We go great, we'll issue
the I/O to the disk.

00:33:25.276 --> 00:33:29.916 A:middle
But we are going to have you
wait for that I/O to come back.

00:33:30.166 --> 00:33:31.856 A:middle
Then we are going to
bring up another thread,

00:33:31.856 --> 00:33:35.876 A:middle
execute the next block and so
on as threads wait we'll bring

00:33:35.876 --> 00:33:37.256 A:middle
up another thread to
execute the next block

00:33:37.256 --> 00:33:38.956 A:middle
on that queue while there's
still work to be done.

00:33:39.256 --> 00:33:43.936 A:middle
The problem here, if I only have
four blocks, that works great.

00:33:44.266 --> 00:33:47.906 A:middle
If I have lots of blocks
and they all want to wait,

00:33:47.906 --> 00:33:50.126 A:middle
we can get what we
call thread explosion.

00:33:51.596 --> 00:33:53.586 A:middle
This is, of course,
a little inefficient.

00:33:53.706 --> 00:33:56.646 A:middle
There are lots of threads
all using resources.

00:33:57.346 --> 00:33:59.176 A:middle
If they all stop
waiting at the same time,

00:33:59.176 --> 00:34:00.496 A:middle
we have lots of contention.



00:33:59.176 --> 00:34:00.496 A:middle
we have lots of contention.

00:34:01.306 --> 00:34:03.576 A:middle
So this isn't great
for performance.

00:34:03.816 --> 00:34:06.046 A:middle
But it is also a
little dangerous

00:34:06.046 --> 00:34:07.166 A:middle
in that there is a
limit to the number

00:34:07.166 --> 00:34:08.126 A:middle
of threads we can bring up.

00:34:08.596 --> 00:34:11.926 A:middle
When you exhaust the limit we
have a problem of what do we do

00:34:11.926 --> 00:34:13.016 A:middle
with new work that comes in?

00:34:13.746 --> 00:34:14.876 A:middle
This brings in deadlock.

00:34:14.876 --> 00:34:19.846 A:middle
I want to talk about an example
of deadlock we saw in one

00:34:19.846 --> 00:34:21.186 A:middle
of our applications internally.

00:34:21.466 --> 00:34:25.466 A:middle
I hope I can talk
you through this.

00:34:25.466 --> 00:34:28.085 A:middle
This is a great example
of how different parts

00:34:28.085 --> 00:34:30.656 A:middle
of your application
interact in unexpected ways.

00:34:31.206 --> 00:34:36.216 A:middle
We have the main thread and
the main thread has a bunch

00:34:36.216 --> 00:34:38.065 A:middle
of work it wants to do.

00:34:38.065 --> 00:34:41.065 A:middle
It will dispatch async
a whole butch of blocks

00:34:41.065 --> 00:34:42.045 A:middle
onto a concurrent queue.

00:34:42.716 --> 00:34:45.246 A:middle
We start bringing up
threads for those blocks,

00:34:45.596 --> 00:34:47.416 A:middle
and those blocks
immediately turn around

00:34:47.416 --> 00:34:49.436 A:middle
and dispatch sync back
on to the main thread.

00:34:50.356 --> 00:34:52.136 A:middle
At this point we've brought

00:34:52.136 --> 00:34:53.396 A:middle
up all the threads
we are willing to.

00:34:53.485 --> 00:34:55.226 A:middle
In the simple example
here, it's four.

00:34:55.295 --> 00:34:57.005 A:middle
We hit the thread limit.

00:34:57.005 --> 00:34:58.646 A:middle
We are not going to bring

00:34:58.646 --> 00:35:00.576 A:middle
up any additional threads
for the thread pool.



00:34:58.646 --> 00:35:00.576 A:middle
up any additional threads
for the thread pool.

00:35:03.436 --> 00:35:03.996 A:middle
Okay, fine.

00:35:03.996 --> 00:35:07.156 A:middle
We need the main thread
to become available again

00:35:07.156 --> 00:35:08.706 A:middle
so that those blocks
can acquire it,

00:35:09.256 --> 00:35:11.146 A:middle
do their work and then complete.

00:35:11.736 --> 00:35:13.206 A:middle
Now it put us back
under our limit.

00:35:14.646 --> 00:35:17.126 A:middle
That might happen some some
situations, but let's imagine

00:35:17.716 --> 00:35:19.676 A:middle
that our main thread then goes

00:35:19.676 --> 00:35:21.516 A:middle
into the dispatch
async to a serial cue.

00:35:22.216 --> 00:35:23.656 A:middle
So far so good.

00:35:24.086 --> 00:35:25.526 A:middle
That block will not execute

00:35:25.526 --> 00:35:28.266 A:middle
yet because there's no
additional threads available

00:35:28.266 --> 00:35:29.436 A:middle
to execute that block.

00:35:29.746 --> 00:35:33.016 A:middle
It will sit there waiting for
one of the threads to return

00:35:33.016 --> 00:35:33.926 A:middle
so we can use it again.

00:35:35.116 --> 00:35:38.336 A:middle
But then our main thread
decides to dispatch sync

00:35:38.486 --> 00:35:39.726 A:middle
to that same serial queue.

00:35:41.216 --> 00:35:43.486 A:middle
The problem is that there
are no threads available

00:35:43.486 --> 00:35:44.256 A:middle
for that serial queue.

00:35:44.746 --> 00:35:48.016 A:middle
The main call is going
to block forever.

00:35:48.546 --> 00:35:50.586 A:middle
This is the classic
deadlock situation.

00:35:51.166 --> 00:35:53.036 A:middle
Our main thread is
waiting on a resource.

00:35:53.686 --> 00:35:55.326 A:middle
In this case a thread
from our thread pool.

00:35:55.326 --> 00:35:57.796 A:middle
Where all the threads from
the thread pool are waiting

00:35:57.796 --> 00:35:59.086 A:middle
on a resource, the main thread.

00:35:59.516 --> 00:36:01.386 A:middle
They're both waiting on each
other and neither is going



00:35:59.516 --> 00:36:01.386 A:middle
They're both waiting on each
other and neither is going

00:36:01.386 --> 00:36:03.866 A:middle
to give up that resource
so we have a deadlock.

00:36:05.146 --> 00:36:07.346 A:middle
This may seem bizarre
and contrived.

00:36:07.346 --> 00:36:09.016 A:middle
But when you have lots
of different parts

00:36:09.016 --> 00:36:10.626 A:middle
of your appliation,
different modules,

00:36:10.666 --> 00:36:12.716 A:middle
frameworks all doing
things at the same time,

00:36:13.236 --> 00:36:15.086 A:middle
it's easier to hit
than you might imagine

00:36:15.816 --> 00:36:18.226 A:middle
and may be more complicated
in practice.

00:36:18.846 --> 00:36:21.096 A:middle
So it's something you
want to keep in mind

00:36:21.096 --> 00:36:22.436 A:middle
as you are working with GCD.

00:36:22.476 --> 00:36:24.696 A:middle
To make sure you can
avoid the situation.

00:36:25.086 --> 00:36:25.956 A:middle
It is easy.

00:36:26.076 --> 00:36:29.176 A:middle
I'll talk through some ways that
we can architect our application

00:36:29.176 --> 00:36:30.666 A:middle
to be resilient against
this problem.

00:36:31.156 --> 00:36:33.836 A:middle
First some basic things.

00:36:34.376 --> 00:36:35.336 A:middle
Always good advice

00:36:35.336 --> 00:36:37.216 A:middle
to use asynchronous
APIs whenever possible,

00:36:37.376 --> 00:36:38.516 A:middle
especially for I/Os.

00:36:39.286 --> 00:36:42.496 A:middle
If you do that, you can
avoid having blocks wait

00:36:42.496 --> 00:36:45.346 A:middle
and as a result you won't
have to bring up more threads.

00:36:45.346 --> 00:36:46.356 A:middle
Gain some efficiency.

00:36:47.426 --> 00:36:50.326 A:middle
You can also use serial queues
for something like this.

00:36:50.526 --> 00:36:52.236 A:middle
If we dispatched all that work

00:36:52.236 --> 00:36:56.366 A:middle
on a serial queue we wouldn't
have had thread explosion.

00:36:56.366 --> 00:36:59.106 A:middle
We would have only executed
one block at a time.

00:36:59.106 --> 00:37:01.956 A:middle
I'm not telling you to serialize
your entire application,



00:36:59.106 --> 00:37:01.956 A:middle
I'm not telling you to serialize
your entire application,

00:37:02.616 --> 00:37:05.526 A:middle
but as you are building
your application

00:37:05.526 --> 00:37:08.486 A:middle
and creating different queues,
service different modules

00:37:08.486 --> 00:37:11.706 A:middle
in your application, unless you
know that you need to run things

00:37:11.706 --> 00:37:13.796 A:middle
in parallel for that
particular module,

00:37:14.046 --> 00:37:16.026 A:middle
in order to meet your
performance goals,

00:37:16.396 --> 00:37:18.526 A:middle
consider starting out with
a serial queue instead.

00:37:19.246 --> 00:37:21.536 A:middle
There's a lot of performance
to be gained from running parts

00:37:21.536 --> 00:37:24.746 A:middle
of your application
concurrently on serial queues.

00:37:25.116 --> 00:37:29.136 A:middle
Then you can profile
your application,

00:37:29.366 --> 00:37:31.366 A:middle
see what needs the additional
performance of running work

00:37:31.366 --> 00:37:34.576 A:middle
in parallel and specifically
design those pieces of work

00:37:34.896 --> 00:37:37.526 A:middle
in ways that are well managed
to avoid thread explosion.

00:37:38.446 --> 00:37:41.526 A:middle
Now, of course, you can
also use NSOperation queues

00:37:41.526 --> 00:37:42.856 A:middle
which have a concurrency limit.

00:37:44.056 --> 00:37:47.406 A:middle
And finally don't
generate unlimited work.

00:37:47.706 --> 00:37:50.176 A:middle
If you can design your work
to be well bounded in terms

00:37:50.176 --> 00:37:51.426 A:middle
of the number of
blocks you need,

00:37:51.756 --> 00:37:52.886 A:middle
that can avoid thread explosion.

00:37:53.906 --> 00:37:56.866 A:middle
Let's look through more
specific code examples

00:37:57.476 --> 00:37:58.526 A:middle
of things that went wrong here.



00:38:00.056 --> 00:38:03.326 A:middle
The first is that we
mixed sync and async

00:38:04.546 --> 00:38:08.566 A:middle
so if I just do a dispatch sync
to a queue, it's really fast.

00:38:08.786 --> 00:38:10.816 A:middle
It is basically taking a lock.

00:38:11.886 --> 00:38:14.766 A:middle
If I do a dispatch
async, it's really fast,

00:38:14.826 --> 00:38:16.216 A:middle
it's an atomic enqueue.

00:38:17.046 --> 00:38:19.756 A:middle
If I have a queue and
I only do one of these,

00:38:19.756 --> 00:38:23.506 A:middle
the performance will be
similar to those primitives.

00:38:24.786 --> 00:38:29.006 A:middle
If I mix these and async to to
a queue and do a dispatch sync,

00:38:29.626 --> 00:38:32.356 A:middle
that dispatch sync has wait
for a thread to be created

00:38:32.456 --> 00:38:35.166 A:middle
and then execute that block, and
then for the block to complete.

00:38:36.036 --> 00:38:38.446 A:middle
Now we have the thread
creation time coming

00:38:38.446 --> 00:38:40.156 A:middle
in to what really would
have been just a lock.

00:38:40.156 --> 00:38:44.056 A:middle
Now, it's absolutely safe to
mix these primitives, but,

00:38:44.246 --> 00:38:46.956 A:middle
as you design the application,
think about whether you need to.

00:38:47.556 --> 00:38:48.606 A:middle
Be especially careful

00:38:48.606 --> 00:38:52.666 A:middle
when mixing them
from the main thread.

00:38:53.406 --> 00:38:56.696 A:middle
Now, of course, the next problem
is that we try to dispatch a lot

00:38:56.696 --> 00:38:58.936 A:middle
of blocks on to a
concurrent queue all at once.



00:39:00.016 --> 00:39:05.186 A:middle
If you do this, and in our
case we are just going to try

00:39:05.186 --> 00:39:06.866 A:middle
to continue executing
off the main thread,

00:39:07.336 --> 00:39:09.686 A:middle
but imagine you do something
similar where you async

00:39:09.686 --> 00:39:10.796 A:middle
and then do a barrier sync.

00:39:11.486 --> 00:39:15.606 A:middle
Either one is dangerous because
it could cause thread explosion

00:39:15.646 --> 00:39:16.386 A:middle
and deadlocks.

00:39:17.266 --> 00:39:19.506 A:middle
But we have a primitive
called dispatch apply,

00:39:19.976 --> 00:39:23.346 A:middle
and these two pieces of code
here are basically exactly the

00:39:23.346 --> 00:39:25.726 A:middle
same from the perspective
of the semantics for you.

00:39:26.306 --> 00:39:28.576 A:middle
By switching to dispatch
apply, you allow GCD

00:39:28.576 --> 00:39:32.026 A:middle
to manage the parallelism and
avoid the thread explosion.

00:39:33.536 --> 00:39:38.376 A:middle
Of course, you can also
use dispatch semaphores.

00:39:38.376 --> 00:39:41.336 A:middle
Many of you are familiar with
the use of semaphores as locks.

00:39:42.126 --> 00:39:43.326 A:middle
We are going to use
the semaphore

00:39:43.406 --> 00:39:44.846 A:middle
as a counting semaphore instead.

00:39:45.436 --> 00:39:47.866 A:middle
We start out by initializing
it with the number

00:39:47.866 --> 00:39:50.296 A:middle
of concurrent tasks
we want to execute.

00:39:50.296 --> 00:39:51.986 A:middle
Say we want four running.

00:39:53.376 --> 00:39:57.476 A:middle
Every time our block completes,
it signals the semaphore.

00:39:58.166 --> 00:40:01.016 A:middle
Every time we submit, we
wait on the semaphore.



00:39:58.166 --> 00:40:01.016 A:middle
Every time we submit, we
wait on the semaphore.

00:40:01.926 --> 00:40:05.686 A:middle
As a result our submitter
thread will do four submissions

00:40:05.766 --> 00:40:08.596 A:middle
and then block on that
semaphore wait until one

00:40:08.596 --> 00:40:10.516 A:middle
of them can complete and
signal the semaphore.

00:40:11.436 --> 00:40:13.376 A:middle
This pattern is nice if
you might be submitting

00:40:13.376 --> 00:40:15.026 A:middle
from multiple places
in your application.

00:40:15.176 --> 00:40:17.956 A:middle
Something like dispatch
apply isn't appropriate.

00:40:22.696 --> 00:40:25.276 A:middle
So hopefully that's helped
you understand a little bit

00:40:25.276 --> 00:40:28.206 A:middle
about thread explosion
and how to avoid it.

00:40:28.206 --> 00:40:30.416 A:middle
I also want to talk
briefly about crash reports.

00:40:31.966 --> 00:40:33.876 A:middle
Unfortunately most of
you probably had to deal

00:40:33.876 --> 00:40:35.086 A:middle
with a crash report
at some point.

00:40:35.456 --> 00:40:36.876 A:middle
There's a lot of
information here.

00:40:37.436 --> 00:40:39.496 A:middle
This is especially true
if you are using GCD.

00:40:39.496 --> 00:40:46.226 A:middle
As you have more threads,
there's more things to parse

00:40:46.596 --> 00:40:47.876 A:middle
and to understand
what's going on.

00:40:49.386 --> 00:40:51.316 A:middle
So I want to talk you
through a couple stacks

00:40:51.636 --> 00:40:55.846 A:middle
that can help you understand
what the different threads

00:40:55.846 --> 00:40:56.806 A:middle
in your application are doing.

00:40:57.216 --> 00:40:58.926 A:middle
So the first one is
the manager thread.

00:40:59.836 --> 00:41:02.146 A:middle
So the manager thread is
something that you are going



00:40:59.836 --> 00:41:02.146 A:middle
So the manager thread is
something that you are going

00:41:02.146 --> 00:41:04.486 A:middle
to see in almost all
GCD using applications.

00:41:04.856 --> 00:41:06.776 A:middle
It's there to help
process dispatch sources.

00:41:07.176 --> 00:41:10.076 A:middle
You notice dispatch manager
thread is the root frame.

00:41:10.736 --> 00:41:12.506 A:middle
You can generally ignore it.

00:41:13.056 --> 00:41:15.366 A:middle
We have idle GCD threads.

00:41:16.006 --> 00:41:17.556 A:middle
These are idle threads
in the thread pool.

00:41:17.866 --> 00:41:19.356 A:middle
You can see start
work queue thread

00:41:19.356 --> 00:41:20.266 A:middle
at the bottom of the stack.

00:41:20.666 --> 00:41:22.216 A:middle
There's an indication
it's a GCD thread.

00:41:22.676 --> 00:41:24.776 A:middle
Work queue current return
indicates it's sitting

00:41:24.776 --> 00:41:25.196 A:middle
there idle.

00:41:25.676 --> 00:41:30.586 A:middle
An active GCD thread, on the
other hand, will still begin

00:41:30.586 --> 00:41:33.016 A:middle
with start work queue thread
but you'll see something

00:41:33.016 --> 00:41:35.746 A:middle
like dispatch client call
out and dispatch call block

00:41:35.746 --> 00:41:37.696 A:middle
and release, followed
by your code.

00:41:38.206 --> 00:41:40.536 A:middle
You'll also see the dispatch
queue name that you passed

00:41:40.536 --> 00:41:41.426 A:middle
when you created the queue.

00:41:41.796 --> 00:41:43.956 A:middle
It's important to give
descriptive queue names.

00:41:44.536 --> 00:41:49.366 A:middle
The main thread when
idle you'll see sitting

00:41:49.366 --> 00:41:53.806 A:middle
in mock message trap, CF run
loop port and CF run loop run

00:41:54.536 --> 00:41:57.346 A:middle
and you'll see com.apple.main
thread.

00:41:57.346 --> 00:42:01.586 A:middle
On the other hand, if
your main queue is active,



00:41:57.346 --> 00:42:01.586 A:middle
On the other hand, if
your main queue is active,

00:42:02.126 --> 00:42:05.246 A:middle
you might see CF run loop is
servicing the main dispatch

00:42:05.326 --> 00:42:09.186 A:middle
queue if it was active because
of the main queue GCD queue.

00:42:09.186 --> 00:42:12.506 A:middle
And in this case which
have NSBlock operation

00:42:12.506 --> 00:42:13.256 A:middle
that we're calling out.

00:42:13.256 --> 00:42:17.226 A:middle
Now, of course, you shouldn't
rely on things not changing.

00:42:17.226 --> 00:42:19.756 A:middle
There are internal details and
I'm walking through this today

00:42:19.756 --> 00:42:23.076 A:middle
to give you a guide
through your crash reports.

00:42:23.506 --> 00:42:25.596 A:middle
Hopefully it will provide
some useful context.

00:42:27.006 --> 00:42:31.376 A:middle
So with that to wrap things
up, remember that an efficient

00:42:31.376 --> 00:42:33.626 A:middle
and responsive application
must be able to adopt

00:42:33.626 --> 00:42:36.436 A:middle
to diverse environments, whether
it's the Watch or Mac Pro.

00:42:36.886 --> 00:42:40.146 A:middle
These different platforms have a
variety of resources available.

00:42:40.606 --> 00:42:43.156 A:middle
GCD is a great way to help
you manage them appropriately.

00:42:43.716 --> 00:42:47.016 A:middle
QoS classes allow
the operating system

00:42:47.016 --> 00:42:49.886 A:middle
to marshall your resources
in the most efficient way.

00:42:51.516 --> 00:42:54.086 A:middle
So you should go home and think
about how you integrate Quality

00:42:54.086 --> 00:42:55.936 A:middle
of Service classes
into your application

00:42:55.936 --> 00:42:58.956 A:middle
and the existing uses of GCD.

00:42:59.486 --> 00:43:02.216 A:middle
Finally, consider how
your apps use GCD and try



00:42:59.486 --> 00:43:02.216 A:middle
Finally, consider how
your apps use GCD and try

00:43:02.216 --> 00:43:03.316 A:middle
to avoid thread explosion.

00:43:04.826 --> 00:43:05.966 A:middle
For more information, check

00:43:05.966 --> 00:43:07.466 A:middle
out the concurrency
programming guide

00:43:07.896 --> 00:43:10.716 A:middle
or the energy efficiency
guides for Mac and iOS apps.

00:43:10.756 --> 00:43:13.036 A:middle
The iOS app one is
new this week.

00:43:13.576 --> 00:43:14.006 A:middle
Check it out.

00:43:14.006 --> 00:43:14.366 A:middle
It's great.

00:43:14.366 --> 00:43:19.146 A:middle
We have the developer forums
and Paul, our evangelist.

00:43:19.416 --> 00:43:21.396 A:middle
A couple related sessions:

00:43:21.766 --> 00:43:26.546 A:middle
Achieving all day battery life
provides more context behind the

00:43:26.546 --> 00:43:27.736 A:middle
energy topics I mentioned.

00:43:28.426 --> 00:43:31.506 A:middle
Optimizing your app
for multitasking iOS 9,

00:43:32.916 --> 00:43:36.846 A:middle
advanced NSOperations, and
performans on iOS and Watch OS,

00:43:37.196 --> 00:43:38.926 A:middle
just after this in Presidio.

00:43:38.926 --> 00:43:42.186 A:middle
If those are new to
you, I highly recommend

00:43:42.186 --> 00:43:44.406 A:middle
that you check those out.

