
00:00:27.156 --> 00:00:28.336 A:middle
&gt;&gt; ERIC BIENVILLE: Good morning.

00:00:29.516 --> 00:00:32.746 A:middle
[ Applause ]

00:00:33.246 --> 00:00:35.716 A:middle
Welcome to the vector
numerics group session.

00:00:36.116 --> 00:00:38.426 A:middle
My name is Eric Bienville
and I'm

00:00:38.536 --> 00:00:40.726 A:middle
with the vector numerics group.

00:00:41.166 --> 00:00:46.656 A:middle
Our group is providing
the accelerate framework

00:00:46.696 --> 00:00:51.726 A:middle
which you already know and
some libraries in the system.

00:00:53.386 --> 00:00:56.176 A:middle
In the accelerate framework,
you will find the image

00:00:56.416 --> 00:01:03.496 A:middle
which is a collection
of hundreds of functions



00:00:56.416 --> 00:01:03.496 A:middle
which is a collection
of hundreds of functions

00:01:04.166 --> 00:01:06.866 A:middle
for image manipulation.

00:01:07.306 --> 00:01:11.456 A:middle
You will also find vDSP
inside accelerate which is

00:01:11.456 --> 00:01:14.436 A:middle
for signal processing
and three layers

00:01:14.436 --> 00:01:17.116 A:middle
of linear algebra libraries.

00:01:18.106 --> 00:01:24.416 A:middle
Outside of accelerate, we also
maintain the Math library, LibM,

00:01:25.076 --> 00:01:27.966 A:middle
and the string functions.

00:01:28.656 --> 00:01:33.136 A:middle
Last year we also introduced
SIMD which is a set of headers

00:01:33.136 --> 00:01:37.406 A:middle
and functions providing
vector types directly mapping

00:01:37.406 --> 00:01:43.516 A:middle
to CPU vector units
near SSC and AVX.

00:01:43.706 --> 00:01:47.796 A:middle
Today we present you three new
additions to these functions.

00:01:48.246 --> 00:01:54.106 A:middle
The first one, compression, is
a library for data compression;

00:01:54.896 --> 00:01:58.076 A:middle
then Steve will present
addition to the SIMD library,

00:01:59.146 --> 00:02:01.296 A:middle
and finally Luke will
present sparse BLAS,



00:01:59.146 --> 00:02:01.296 A:middle
and finally Luke will
present sparse BLAS,

00:02:02.196 --> 00:02:04.206 A:middle
linear algebra for
sparse matrices.

00:02:05.376 --> 00:02:07.116 A:middle
Let's start with compression.

00:02:08.256 --> 00:02:09.246 A:middle
What is that?

00:02:10.596 --> 00:02:14.666 A:middle
Compression is a new
library providing a unified

00:02:14.766 --> 00:02:17.766 A:middle
and simple API for
lossless data compression.

00:02:18.426 --> 00:02:22.406 A:middle
Why would you need such a
thing where if you already try

00:02:22.406 --> 00:02:25.496 A:middle
to use compression in
your app, you first have

00:02:25.546 --> 00:02:30.346 A:middle
to pick a compression
somewhere, read the manual,

00:02:30.346 --> 00:02:32.146 A:middle
write the code, test it.

00:02:32.146 --> 00:02:35.276 A:middle
If it doesn't work, you need

00:02:35.276 --> 00:02:38.036 A:middle
to pick another one,
write the code again.

00:02:38.486 --> 00:02:39.446 A:middle
That's a lot of trouble.

00:02:39.946 --> 00:02:44.286 A:middle
And also sometimes, well,
actually usually you would have

00:02:44.346 --> 00:02:46.786 A:middle
to include the compression
curve inside your app

00:02:47.156 --> 00:02:48.996 A:middle
which is a maintenance
nightmare.

00:02:49.646 --> 00:02:51.326 A:middle
So the idea is to
provide a wrapper

00:02:51.326 --> 00:02:57.406 A:middle
to common compression algorithms
and their unified API and also

00:02:57.406 --> 00:02:59.726 A:middle
with some benefits
like optimized code



00:03:00.596 --> 00:03:02.966 A:middle
and easy switch between
algorithms.

00:03:03.606 --> 00:03:06.306 A:middle
Let's start with the
algorithms we put inside.

00:03:07.676 --> 00:03:11.616 A:middle
When we speak about compression
algorithms there are two metrics

00:03:11.616 --> 00:03:12.126 A:middle
to consider.

00:03:12.126 --> 00:03:15.446 A:middle
The first one is
obviously compression ratio.

00:03:15.896 --> 00:03:18.336 A:middle
That is the ratio
between raw data size

00:03:18.856 --> 00:03:23.226 A:middle
and the compressed payload
size, and then we are interested

00:03:23.226 --> 00:03:26.266 A:middle
in the encoding speed
and the decoding speed.

00:03:27.086 --> 00:03:32.386 A:middle
So to select algorithm, we
put them in one single graph.

00:03:34.096 --> 00:03:37.096 A:middle
This graph, so on the X axis
you have the compression ratio,

00:03:38.136 --> 00:03:40.896 A:middle
and on the Y axis,
the encoding speed.

00:03:40.896 --> 00:03:46.716 A:middle
And in the middle there is a
reference, reference compressor.

00:03:46.906 --> 00:03:49.596 A:middle
So all of the algorithms are
compared to this reference.

00:03:51.146 --> 00:03:53.236 A:middle
So if you are on the
right, you compress better,

00:03:53.356 --> 00:03:56.776 A:middle
if you are on top, you
compress faster, and that's it.

00:03:56.996 --> 00:03:59.056 A:middle
So let's put some
algorithms inside here.



00:04:00.016 --> 00:04:02.206 A:middle
The center one is zlib.

00:04:02.306 --> 00:04:04.456 A:middle
That's the most used.

00:04:05.316 --> 00:04:06.646 A:middle
And so what did we do?

00:04:06.646 --> 00:04:11.896 A:middle
We chose the best compression,
this one, right, this one, LZMA,

00:04:11.896 --> 00:04:15.936 A:middle
and the fastest one LZ4.

00:04:15.936 --> 00:04:22.266 A:middle
You will see the gray line is
to show how it becomes more

00:04:22.266 --> 00:04:25.076 A:middle
and more harder when you
want more compression.

00:04:25.736 --> 00:04:27.896 A:middle
So more compression means slower

00:04:28.116 --> 00:04:32.646 A:middle
and less compression means
faster, exponentially so.

00:04:33.586 --> 00:04:36.596 A:middle
And if you look at that closely,
there is a first point here

00:04:37.886 --> 00:04:42.196 A:middle
which is above zlib, that means
faster and a bit to the right,

00:04:42.196 --> 00:04:45.926 A:middle
that means compresses better
that's LZFSE that's a new

00:04:45.926 --> 00:04:47.656 A:middle
compressor we're
introducing today

00:04:49.356 --> 00:04:53.496 A:middle
and a good alternative to zlib.

00:04:53.496 --> 00:04:56.506 A:middle
I will talk about that later.

00:04:56.736 --> 00:04:58.356 A:middle
These are the four
compressors we put

00:04:58.436 --> 00:05:00.136 A:middle
in the compression library.



00:04:58.436 --> 00:05:00.136 A:middle
in the compression library.

00:05:00.136 --> 00:05:05.606 A:middle
And the decode looks like
that so that's the same thing,

00:05:05.696 --> 00:05:08.146 A:middle
if you compress more,
you will need more time

00:05:08.146 --> 00:05:09.826 A:middle
to decompress also.

00:05:11.136 --> 00:05:15.476 A:middle
And our four compressors are
at the same position here.

00:05:16.036 --> 00:05:18.816 A:middle
So these are the four
algorithms we selected to put

00:05:18.816 --> 00:05:20.306 A:middle
in the compression library.

00:05:21.166 --> 00:05:25.446 A:middle
LZMA for high compression,
but it's low.

00:05:25.806 --> 00:05:29.926 A:middle
LZ4 for very fast compression,
but it doesn't compress

00:05:29.926 --> 00:05:35.596 A:middle
that much, and in the middle you
have zlib and LZFSE for balance

00:05:35.596 --> 00:05:37.556 A:middle
between compression
ratio and speed.

00:05:38.256 --> 00:05:42.116 A:middle
So I'm matching different
use cases

00:05:42.116 --> 00:05:44.586 A:middle
like for software distribution
you want to use LZMA

00:05:44.586 --> 00:05:48.166 A:middle
because you will compress
once on the server and ship

00:05:48.166 --> 00:05:51.436 A:middle
that so you want the shipping
part to be as small as possible.

00:05:52.096 --> 00:05:54.656 A:middle
And then what did we do?

00:05:54.986 --> 00:05:58.326 A:middle
We optimized some of
them for Apple hardware.

00:05:58.766 --> 00:05:59.926 A:middle
What does that mean?



00:06:00.716 --> 00:06:01.176 A:middle
Like that.

00:06:01.176 --> 00:06:04.726 A:middle
So, for example, we
optimized zlib decoder,

00:06:04.726 --> 00:06:08.736 A:middle
and the performance of the
zlib decoder we provide is

00:06:08.736 --> 00:06:15.246 A:middle
compression is almost 1.6X
faster than the normal zlib.

00:06:16.006 --> 00:06:17.176 A:middle
And the same goes with the LZ4.

00:06:17.176 --> 00:06:20.926 A:middle
So that's another benefit.

00:06:20.926 --> 00:06:23.166 A:middle
And you will also benefit
from security of data

00:06:23.166 --> 00:06:27.496 A:middle
and performance update without
having to change your code.

00:06:27.956 --> 00:06:31.576 A:middle
Let me talk briefly about
LZFSE, our new compressor.

00:06:32.306 --> 00:06:37.216 A:middle
Why would would you
need a new compressor?

00:06:37.216 --> 00:06:39.816 A:middle
First, its fun.

00:06:40.426 --> 00:06:44.006 A:middle
And then, so why are
we optimizing zlib?

00:06:44.056 --> 00:06:49.656 A:middle
We came to the conclusion in
the entropy part of the zlib,

00:06:50.076 --> 00:06:54.816 A:middle
it is a bottle neck and there
is not much we can do to solve

00:06:54.816 --> 00:06:58.526 A:middle
that except replacing
it with something else.

00:06:58.526 --> 00:06:59.436 A:middle
So that's what we did.



00:07:00.126 --> 00:07:03.816 A:middle
We took the new technology
called finer state entropy

00:07:03.936 --> 00:07:06.796 A:middle
and replaced the part
of zlib with that.

00:07:07.326 --> 00:07:13.136 A:middle
And the good news is that
LZFSE is able to map better

00:07:13.136 --> 00:07:17.356 A:middle
on modern CPU architecture,
much better.

00:07:18.686 --> 00:07:24.166 A:middle
And the design goal of that was,
match the zlib compression ratio

00:07:24.166 --> 00:07:26.016 A:middle
and make this thing
as fast as we can.

00:07:26.796 --> 00:07:30.146 A:middle
So let me show you
numbers about that.

00:07:30.546 --> 00:07:32.166 A:middle
That's compression ratio.

00:07:32.256 --> 00:07:35.266 A:middle
So by design it's
the same thing.

00:07:37.076 --> 00:07:42.276 A:middle
This is energy efficiency,
both encode and decode, LZFSE.

00:07:43.366 --> 00:07:46.606 A:middle
As you can see we are
2-point something times faster

00:07:46.606 --> 00:07:50.956 A:middle
for encode and 2.5
times faster for decode,

00:07:50.956 --> 00:07:52.186 A:middle
not faster, more efficient.

00:07:52.306 --> 00:07:53.486 A:middle
What does that mean?

00:07:54.076 --> 00:07:55.686 A:middle
Suppose you have a
full battery charge

00:07:55.686 --> 00:07:58.116 A:middle
and nothing is compressing.

00:07:59.236 --> 00:08:01.096 A:middle
With zlib you will
compress, let's say,



00:07:59.236 --> 00:08:01.096 A:middle
With zlib you will
compress, let's say,

00:08:01.096 --> 00:08:03.936 A:middle
5 terabytes of data before
the battery is empty.

00:08:04.436 --> 00:08:08.746 A:middle
With LZFSE you would be able to
compress 12 terabytes with data

00:08:08.956 --> 00:08:09.976 A:middle
with the same battery charge.

00:08:10.216 --> 00:08:12.376 A:middle
That's what energy
efficiency means.

00:08:13.896 --> 00:08:15.596 A:middle
Actually speed is
not bad either,

00:08:16.096 --> 00:08:20.086 A:middle
because we can reach 2-point
something times faster

00:08:20.086 --> 00:08:22.496 A:middle
for encoding and three
times faster for decoding

00:08:23.026 --> 00:08:24.216 A:middle
with the same compression ratio.

00:08:25.676 --> 00:08:28.366 A:middle
So that was about LZFSE.

00:08:28.366 --> 00:08:31.376 A:middle
Now, let me show
you to use this API.

00:08:31.566 --> 00:08:32.576 A:middle
There are two APIs.

00:08:32.576 --> 00:08:36.395 A:middle
There is a buffer API to use
when you have all of the data

00:08:36.395 --> 00:08:37.866 A:middle
at once in one single buffer.

00:08:38.626 --> 00:08:41.596 A:middle
And there is streaming
API to use

00:08:41.596 --> 00:08:44.776 A:middle
when receive the data in PCs.

00:08:46.166 --> 00:08:47.706 A:middle
So buffer API is quite simple.

00:08:47.706 --> 00:08:52.896 A:middle
It's like a super mem copy
thing, you give it a buffer

00:08:52.896 --> 00:08:55.976 A:middle
and the number of bytes
inside, destination buffer

00:08:55.976 --> 00:08:59.886 A:middle
and the capacity of it,
and you just give it also



00:09:00.096 --> 00:09:03.666 A:middle
which algorithms
it needs to use.

00:09:03.836 --> 00:09:04.876 A:middle
Here it says LZFSE.

00:09:05.446 --> 00:09:09.046 A:middle
You see that switching between
compressors is changing this

00:09:09.046 --> 00:09:11.036 A:middle
constant, you don't have
any code to rewrite.

00:09:11.036 --> 00:09:15.486 A:middle
The new code is compression
encode buffer function,

00:09:15.866 --> 00:09:18.036 A:middle
and it will return the
number of bytes put

00:09:18.036 --> 00:09:21.666 A:middle
in the destination buffer
of 0 if something went wrong

00:09:21.666 --> 00:09:25.246 A:middle
or there was not enough room
in the destination buffer.

00:09:26.366 --> 00:09:28.666 A:middle
Decoding, that is
the same story.

00:09:29.276 --> 00:09:32.856 A:middle
So you will pass a buffer
containing compressed payload

00:09:33.516 --> 00:09:36.766 A:middle
and a buffer to receive
the decoded data.

00:09:38.726 --> 00:09:43.526 A:middle
The difference here is it will
return the number of bytes put

00:09:43.936 --> 00:09:45.526 A:middle
in the output buffer

00:09:46.026 --> 00:09:50.706 A:middle
and if something fails actually
it will fill up the buffer.

00:09:50.706 --> 00:09:55.046 A:middle
So it will just truncate
the output.

00:09:55.046 --> 00:09:56.496 A:middle
Now, let me show
you the stream API,

00:09:56.586 --> 00:09:58.946 A:middle
which is a little more complex.



00:10:00.146 --> 00:10:02.976 A:middle
Because you will need to have
some state between the calls.

00:10:03.056 --> 00:10:06.776 A:middle
So we will call the library
multiple times with new pieces

00:10:06.776 --> 00:10:08.486 A:middle
of raw data to compress.

00:10:08.526 --> 00:10:10.556 A:middle
It's like a candy factory.

00:10:10.556 --> 00:10:13.306 A:middle
You send sugar inside and
get candies at the output.

00:10:13.436 --> 00:10:19.236 A:middle
So you will have the
stream object to initialize

00:10:19.236 --> 00:10:21.416 A:middle
and then cause a process
function many times

00:10:22.046 --> 00:10:26.176 A:middle
and at the end you will
have to call a function

00:10:26.176 --> 00:10:29.036 A:middle
to free the resources
used by the stream object.

00:10:30.216 --> 00:10:32.536 A:middle
So let me show you
in detail with code.

00:10:32.536 --> 00:10:35.796 A:middle
So first, we initialize the
stream object, you say okay,

00:10:35.796 --> 00:10:39.056 A:middle
I want to encode, I want
to use LZFSE, initialize.

00:10:39.756 --> 00:10:43.746 A:middle
Then you will have
to call process.

00:10:44.496 --> 00:10:49.746 A:middle
Before calling process usually
in other compression libraries,

00:10:50.356 --> 00:10:53.316 A:middle
you will have to tell it where
are the bytes to compress

00:10:53.316 --> 00:10:55.466 A:middle
and where to put the output.

00:10:56.226 --> 00:11:00.606 A:middle
And then you call process,
and it will, as the consumer,



00:10:56.226 --> 00:11:00.606 A:middle
And then you call process,
and it will, as the consumer,

00:11:00.606 --> 00:11:02.366 A:middle
input or fill out the output

00:11:03.316 --> 00:11:05.856 A:middle
and it will update
these fields for you.

00:11:06.786 --> 00:11:09.996 A:middle
Then at the end, you need
to tell it, okay, I'm done,

00:11:10.046 --> 00:11:12.076 A:middle
no more sugar, finalize.

00:11:13.196 --> 00:11:15.836 A:middle
And after that, you are not
allowed to send more data

00:11:15.906 --> 00:11:18.186 A:middle
to compress, but you may
need to call it several times

00:11:18.216 --> 00:11:20.666 A:middle
to empty the pipes
and get the output.

00:11:21.616 --> 00:11:25.696 A:middle
And at that point it will return
end, and which means in the end,

00:11:26.546 --> 00:11:29.516 A:middle
and you will need to call
compression stream destroy

00:11:29.516 --> 00:11:32.336 A:middle
to free the resources.

00:11:32.336 --> 00:11:32.916 A:middle
So encode.

00:11:32.916 --> 00:11:34.926 A:middle
Decode is actually
a bit simpler.

00:11:35.236 --> 00:11:38.306 A:middle
You need the same
initialization code.

00:11:39.086 --> 00:11:42.456 A:middle
This time that's with
decode instead of encode.

00:11:43.806 --> 00:11:47.246 A:middle
Then you send the data and
it will figure out by itself

00:11:47.246 --> 00:11:48.676 A:middle
if it's the end of
the stream or not.

00:11:48.676 --> 00:11:51.786 A:middle
So at some point it will return
end, which means you got all

00:11:51.786 --> 00:11:55.266 A:middle
of the data, and you
need to destroy again.

00:11:55.266 --> 00:11:56.446 A:middle
And that's it.

00:11:56.996 --> 00:11:58.206 A:middle
We are done with compression.

00:11:59.326 --> 00:12:03.496 A:middle
So let me wrap up, that's a new
library providing a simplified



00:11:59.326 --> 00:12:03.496 A:middle
So let me wrap up, that's a new
library providing a simplified

00:12:03.496 --> 00:12:06.036 A:middle
and unified API on top
of several compressors.

00:12:06.036 --> 00:12:09.976 A:middle
LZMA, LZ4, zlib, and LZFSE,

00:12:10.026 --> 00:12:14.186 A:middle
which is to be preferred
over zlib.

00:12:15.486 --> 00:12:18.336 A:middle
And LZFSE which is our new
high performance compressor.

00:12:18.876 --> 00:12:24.286 A:middle
Thank you and let me call
Steve, who will tell us

00:12:24.286 --> 00:12:26.996 A:middle
about improvements
to the SIMD library.

00:12:28.136 --> 00:12:28.456 A:middle
Thank you.

00:12:29.516 --> 00:12:34.046 A:middle
[ Applause ]

00:12:34.546 --> 00:12:34.966 A:middle
&gt;&gt; STEVE CANON: Thanks, Eric.

00:12:35.726 --> 00:12:36.666 A:middle
My name is Steve Canon.

00:12:37.166 --> 00:12:39.116 A:middle
I'm an engineer on the vector
numerics group with Eric,

00:12:39.706 --> 00:12:42.476 A:middle
and today I will talk about
SIMD, which is a library

00:12:42.476 --> 00:12:43.926 A:middle
for two dimensional,
three dimensional,

00:12:43.926 --> 00:12:47.066 A:middle
and four dimensional
vector math.

00:12:47.066 --> 00:12:52.496 A:middle
We introduced SIMD last year in
Yosemite, and iOS 8 and it works

00:12:52.496 --> 00:12:54.906 A:middle
in C, Objective-C, and C++

00:12:55.286 --> 00:12:57.556 A:middle
and it closely mirrors the
Metal shading language.

00:12:58.036 --> 00:13:01.486 A:middle
That means it's really easy
to write code that will run



00:12:58.036 --> 00:13:01.486 A:middle
That means it's really easy
to write code that will run

00:13:01.486 --> 00:13:04.646 A:middle
on the GPU in Metal and use
the same data structures

00:13:04.646 --> 00:13:06.906 A:middle
and the same code on
the CPU using SIMD.

00:13:06.906 --> 00:13:11.276 A:middle
It closely hews to the tradition
after shading languages

00:13:11.276 --> 00:13:14.156 A:middle
and the way they work
with vectors and matrices.

00:13:14.536 --> 00:13:15.636 A:middle
So what's new this year is

00:13:16.616 --> 00:13:17.946 A:middle
that we have support
for Swift as well.

00:13:18.516 --> 00:13:22.956 A:middle
[ Applause ]

00:13:23.456 --> 00:13:25.816 A:middle
So everything from here on
in the talk or in my piece

00:13:25.816 --> 00:13:27.106 A:middle
of the talk is going
to be Swift,

00:13:27.176 --> 00:13:28.186 A:middle
all of the examples are Swift.

00:13:28.686 --> 00:13:32.596 A:middle
For the most part, everything
is about exactly the same

00:13:32.826 --> 00:13:34.986 A:middle
in C, Objective-C and C++.

00:13:35.476 --> 00:13:39.406 A:middle
If you want to see in depth the
story for those languages check

00:13:39.406 --> 00:13:42.366 A:middle
out last year's video session
and there are a few things

00:13:42.366 --> 00:13:43.876 A:middle
where there are important
differences I will call

00:13:43.876 --> 00:13:45.106 A:middle
out explicitly in the talk,

00:13:45.236 --> 00:13:48.266 A:middle
but all of the examples
will be Swift.

00:13:48.576 --> 00:13:50.996 A:middle
Why did we need to introduce
a new vector library?

00:13:51.166 --> 00:13:53.736 A:middle
There were a bunch of vector
libraries on the platform

00:13:53.736 --> 00:13:55.886 A:middle
and I have worked
on a bunch of them,

00:13:55.886 --> 00:13:57.136 A:middle
so why do we add a new one?

00:13:57.166 --> 00:13:59.786 A:middle
I will show you examples
of a few things

00:13:59.786 --> 00:14:01.246 A:middle
that were a little
bit inconvenient



00:13:59.786 --> 00:14:01.246 A:middle
that were a little
bit inconvenient

00:14:01.506 --> 00:14:02.846 A:middle
with other vector libraries

00:14:02.846 --> 00:14:04.716 A:middle
and how they are much
nicer using SIMD.

00:14:06.196 --> 00:14:07.956 A:middle
BLAS is a great vector library.

00:14:08.106 --> 00:14:09.056 A:middle
It's part of the accelerate.

00:14:09.056 --> 00:14:10.486 A:middle
It's one the first
things I worked on.

00:14:10.596 --> 00:14:15.206 A:middle
Here is an example of
multiplying a vector by a scaler

00:14:15.206 --> 00:14:17.296 A:middle
and adding it to
another vector in BLAS.

00:14:17.956 --> 00:14:20.316 A:middle
So we create two Swift arrays.

00:14:20.676 --> 00:14:21.976 A:middle
Those will be our vectors.

00:14:22.656 --> 00:14:27.966 A:middle
We can call this C BLAS Saxpy
function, which will multiply X

00:14:27.966 --> 00:14:30.686 A:middle
by 2 and add to Y and
store the result in Y.

00:14:31.456 --> 00:14:35.406 A:middle
There is a bunch of other
information we need to pass

00:14:35.506 --> 00:14:38.116 A:middle
because this is an API
that takes raw coiners,

00:14:38.116 --> 00:14:40.486 A:middle
it doesn't know anything
about lengths or strides;

00:14:40.856 --> 00:14:42.176 A:middle
we need to provide
that information.

00:14:42.686 --> 00:14:44.816 A:middle
There is also inefficiency
that comes

00:14:44.816 --> 00:14:46.956 A:middle
from making an explicit
call to do this work.

00:14:46.956 --> 00:14:49.936 A:middle
We will look at GLKit, which
is another great library,

00:14:49.936 --> 00:14:50.666 A:middle
I love GLKit.

00:14:51.456 --> 00:14:53.746 A:middle
It has explicit vector
types, so you don't just have

00:14:53.776 --> 00:14:56.696 A:middle
to use raw arrays, but the
functions are very verbose.

00:14:56.696 --> 00:15:00.696 A:middle
You have to call it GLK
vector 3 multiply scaler



00:14:56.696 --> 00:15:00.696 A:middle
You have to call it GLK
vector 3 multiply scaler

00:15:01.216 --> 00:15:03.956 A:middle
to do your arithmetic
and that's too wordy.

00:15:04.316 --> 00:15:06.046 A:middle
Here is what it looks
like using SIMD.

00:15:09.476 --> 00:15:10.786 A:middle
This is a lot nicer, right?

00:15:13.196 --> 00:15:16.956 A:middle
You write down the arithmetic
you want to do and it works,

00:15:16.956 --> 00:15:18.526 A:middle
you don't need to
call functions.

00:15:18.886 --> 00:15:19.596 A:middle
Life is nice.

00:15:20.656 --> 00:15:21.866 A:middle
So this is really nice.

00:15:22.206 --> 00:15:23.106 A:middle
What else can you do?

00:15:24.936 --> 00:15:28.916 A:middle
We have vectors of floats,
doubles, and 32 bit integers

00:15:28.966 --> 00:15:30.236 A:middle
of lengths 2, 3, and 4.

00:15:31.126 --> 00:15:35.106 A:middle
In C, Objective-C and C++
there are other vector types

00:15:35.106 --> 00:15:39.476 A:middle
available, for now we
have the subset in Swift,

00:15:39.966 --> 00:15:42.796 A:middle
and the reason we chose this
subset is it's what you have

00:15:42.826 --> 00:15:44.886 A:middle
to use most often
to interoperate

00:15:45.096 --> 00:15:47.396 A:middle
with other libraries for the
Metal programs you are going

00:15:47.396 --> 00:15:49.566 A:middle
to write when you want to
use model I/O, when you want

00:15:49.566 --> 00:15:51.556 A:middle
to do graphics stuff, these
are the types you want

00:15:51.556 --> 00:15:52.106 A:middle
to have available.

00:15:53.606 --> 00:15:54.786 A:middle
What can do you with
these types?

00:15:55.096 --> 00:15:56.716 A:middle
You can create them, first off.

00:15:56.826 --> 00:15:58.986 A:middle
We have a bunch of initializers
that do nice things,

00:15:58.986 --> 00:16:00.036 A:middle
you can create the zero vector,



00:15:58.986 --> 00:16:00.036 A:middle
you can create the zero vector,

00:16:00.636 --> 00:16:03.356 A:middle
you can explicitly specify
the elements of the vector,

00:16:03.676 --> 00:16:06.966 A:middle
you can have a vector with
all of the components equal.

00:16:06.966 --> 00:16:08.646 A:middle
There are lots of
nice initializers.

00:16:09.406 --> 00:16:10.656 A:middle
You can do arithmetic.

00:16:10.896 --> 00:16:12.226 A:middle
You have element-wise
arithmetic,

00:16:12.256 --> 00:16:13.356 A:middle
the operators just work.

00:16:13.356 --> 00:16:16.926 A:middle
So if I multiply two vectors I
get a new vector, each element

00:16:16.926 --> 00:16:20.096 A:middle
of which, the value in
that element, is the result

00:16:20.096 --> 00:16:22.616 A:middle
of multiplying the corresponding
elements of the input vectors.

00:16:23.006 --> 00:16:26.346 A:middle
Divide, same thing, I
can multiply by a scaler.

00:16:26.896 --> 00:16:29.506 A:middle
I can do a dot product,
cross product, et cetera.

00:16:29.556 --> 00:16:31.356 A:middle
I already showed
you one example.

00:16:31.466 --> 00:16:33.876 A:middle
I will show you another
simple example right now.

00:16:34.236 --> 00:16:36.316 A:middle
This is a reflect
function that I might use

00:16:36.316 --> 00:16:37.466 A:middle
in graphics frequently.

00:16:37.916 --> 00:16:41.216 A:middle
It takes 1 vector
X and it reflects X

00:16:41.526 --> 00:16:43.936 A:middle
through the plain perpendicular
to the normal vector N,

00:16:45.046 --> 00:16:47.556 A:middle
and to write this we write down
the mathematical expression.

00:16:47.966 --> 00:16:52.986 A:middle
Write X minus twice the dot
product of X and N times N,

00:16:53.936 --> 00:16:56.796 A:middle
but you shouldn't actually have
to write this function, right?

00:16:56.796 --> 00:16:58.476 A:middle
It should be available
to you already.

00:16:59.296 --> 00:17:00.226 A:middle
And it is.



00:16:59.296 --> 00:17:00.226 A:middle
And it is.

00:17:00.286 --> 00:17:02.466 A:middle
We have a bunch of
geometry, shader,

00:17:02.466 --> 00:17:03.686 A:middle
and math functions available.

00:17:04.286 --> 00:17:07.175 A:middle
So we have dot product, cross
product, reflect, refract,

00:17:07.356 --> 00:17:10.226 A:middle
distances, et cetera, all of
the stuff you want to use.

00:17:10.226 --> 00:17:12.296 A:middle
If you have written
shader programs before,

00:17:12.425 --> 00:17:14.056 A:middle
you have used these
functions a lot.

00:17:14.056 --> 00:17:17.236 A:middle
This is just a standard
stuff you would use in Metal

00:17:17.236 --> 00:17:21.596 A:middle
or open CL or GLSL or whatever
your favorite shader language

00:17:21.596 --> 00:17:25.796 A:middle
is, and we also have a variety
of math functions available

00:17:25.796 --> 00:17:29.826 A:middle
from accelerate for the float
4 type so you can use V sign F,

00:17:29.896 --> 00:17:33.396 A:middle
VCosf, do Math functions
with these types.

00:17:34.156 --> 00:17:36.216 A:middle
We have matrices
as well as vectors.

00:17:37.646 --> 00:17:40.516 A:middle
Matrix types are float N
by N, and double N by M.

00:17:41.206 --> 00:17:43.996 A:middle
So N is the number of columns
and M is the number of rows,

00:17:44.066 --> 00:17:46.406 A:middle
if you are a mathematician,
this is weird.

00:17:46.406 --> 00:17:49.176 A:middle
If you are a graphics
programmer, this is natural.

00:17:49.346 --> 00:17:52.506 A:middle
So you guys will
feel right at home.

00:17:53.076 --> 00:17:55.006 A:middle
N and M can be two, three,
or four, they don't need

00:17:55.006 --> 00:17:56.906 A:middle
to be square matrixes
so, for instance,

00:17:56.906 --> 00:18:02.236 A:middle
a float 2 by 3 is a matrix with
two columns and three rows.



00:17:56.906 --> 00:18:02.236 A:middle
a float 2 by 3 is a matrix with
two columns and three rows.

00:18:03.486 --> 00:18:06.686 A:middle
Again, we have a variety
of initializers for you,

00:18:07.196 --> 00:18:08.686 A:middle
so you can create a zero matrix,

00:18:08.686 --> 00:18:10.136 A:middle
you can create the
identity matrix,

00:18:10.266 --> 00:18:11.486 A:middle
you can create a
diagonal matrix,

00:18:11.486 --> 00:18:14.516 A:middle
by specifying the elements
of the diagonal if you want,

00:18:14.736 --> 00:18:18.246 A:middle
all the elements if you want,
either as an array of arrays

00:18:18.636 --> 00:18:19.866 A:middle
or as an array of vectors.

00:18:23.296 --> 00:18:24.286 A:middle
All sorts of nice things.

00:18:25.116 --> 00:18:29.456 A:middle
And I will show you a little
arithmetic example using

00:18:29.456 --> 00:18:30.306 A:middle
matrices as well.

00:18:31.336 --> 00:18:33.866 A:middle
Let's create a matrix
with 2s on the diagonal.

00:18:33.966 --> 00:18:37.386 A:middle
This is a matrix where when you
multiply it will scale a vector

00:18:37.386 --> 00:18:37.806 A:middle
by 2.

00:18:38.426 --> 00:18:42.306 A:middle
Let's modify the last column
to put some values there.

00:18:42.306 --> 00:18:46.616 A:middle
This is a transformation
matrix that scales vectors by 2

00:18:46.616 --> 00:18:51.286 A:middle
and applies an offset as
well, it translates them.

00:18:52.166 --> 00:18:54.926 A:middle
We can apply this to
a vector of all 1s.

00:18:59.066 --> 00:19:00.856 A:middle
And we can also undo
the transform



00:18:59.066 --> 00:19:00.856 A:middle
And we can also undo
the transform

00:19:00.856 --> 00:19:03.056 A:middle
by using the inverse
property of our matrix

00:19:03.056 --> 00:19:05.486 A:middle
to get the inverse
transformation to get back

00:19:05.526 --> 00:19:06.336 A:middle
to the original vector.

00:19:07.746 --> 00:19:09.586 A:middle
When you want to
interoperate between languages,

00:19:09.586 --> 00:19:12.406 A:middle
you might have Objective-C
API like model I/O APIs

00:19:12.406 --> 00:19:13.666 A:middle
that you want to call.

00:19:15.436 --> 00:19:19.266 A:middle
The Swift vector types
are layout compatible

00:19:19.476 --> 00:19:20.986 A:middle
with the Objective-C
vector types.

00:19:21.176 --> 00:19:22.276 A:middle
What I mean by that is they,

00:19:22.276 --> 00:19:24.226 A:middle
have exactly the same
representation in memory,

00:19:24.626 --> 00:19:26.386 A:middle
and the compiler knows
that they are the same.

00:19:27.466 --> 00:19:32.056 A:middle
So you don't need to do anything
to convert a Swift vector type

00:19:32.056 --> 00:19:34.596 A:middle
into an Objective-C vector type
or an Objective-C vector type

00:19:34.636 --> 00:19:35.556 A:middle
into a Swift vector type.

00:19:35.556 --> 00:19:38.706 A:middle
So here I have an Objective-C
API that returns a vector,

00:19:38.966 --> 00:19:42.716 A:middle
a SIMD vector, and I can
use it immediately in Swift.

00:19:43.066 --> 00:19:46.636 A:middle
For matrices I have to
initialize a Swift matrix

00:19:47.066 --> 00:19:49.106 A:middle
from the Objective-C
matrix that I get.

00:19:49.496 --> 00:19:51.476 A:middle
It's a really cheap
operation, basically a copy

00:19:51.476 --> 00:19:53.726 A:middle
because the layout is
the same, but I do need

00:19:53.726 --> 00:19:54.556 A:middle
to call the initializer.

00:19:55.826 --> 00:19:57.876 A:middle
When I want to pass Swift
types to Objective-C,

00:19:58.176 --> 00:19:59.506 A:middle
the story is exactly the same.



00:20:00.216 --> 00:20:01.696 A:middle
I can pass the vectors

00:20:02.076 --> 00:20:04.336 A:middle
for matrices I use
the C matrix property

00:20:04.636 --> 00:20:07.686 A:middle
to get an Objective-C
matrix that I can pass

00:20:07.756 --> 00:20:08.956 A:middle
to my Objective-C API.

00:20:10.546 --> 00:20:12.356 A:middle
So that's SIMD.

00:20:12.886 --> 00:20:14.266 A:middle
SIMD is great for
when you want to work

00:20:14.266 --> 00:20:16.116 A:middle
with small vectors and matrixes.

00:20:16.246 --> 00:20:19.136 A:middle
Sometimes you want to work with
bigger vectors and matrixes too.

00:20:20.076 --> 00:20:22.436 A:middle
So I will talk to
you about LAPACK,

00:20:22.566 --> 00:20:23.716 A:middle
BLAS, and linear algebra.

00:20:24.096 --> 00:20:27.026 A:middle
This is the fun part of
the talk; we have a break

00:20:27.026 --> 00:20:28.186 A:middle
from the math nerdery.

00:20:30.616 --> 00:20:35.206 A:middle
So LAPACK and BLAS are industry
standard math libraries.

00:20:35.796 --> 00:20:41.306 A:middle
These are some of the oldest
APIs on the platform actually,

00:20:41.496 --> 00:20:43.996 A:middle
which means they are a
little bit cryptic looking

00:20:44.506 --> 00:20:46.446 A:middle
but there is tons of
documentation for them online

00:20:46.446 --> 00:20:48.096 A:middle
because they have been
around for 40 years,

00:20:49.746 --> 00:20:53.286 A:middle
and a lot of times you might
have some code from a library

00:20:53.286 --> 00:20:55.406 A:middle
that you get that depends
on having these available,

00:20:55.636 --> 00:20:57.846 A:middle
just link against
accelerate, it works fine.

00:20:58.156 --> 00:20:59.226 A:middle
It's pretty easy to use.

00:20:59.856 --> 00:21:03.566 A:middle
Linear algebra is a new
interface we introduced last



00:20:59.856 --> 00:21:03.566 A:middle
Linear algebra is a new
interface we introduced last

00:21:03.566 --> 00:21:06.536 A:middle
year that has much simpler APIs

00:21:06.586 --> 00:21:09.016 A:middle
for doing the most common
linear algebra operations.

00:21:09.396 --> 00:21:12.806 A:middle
So this is exactly the same
operation on the last slide,

00:21:12.896 --> 00:21:15.306 A:middle
it's solving a linear
system and rather

00:21:15.336 --> 00:21:18.296 A:middle
than calling a cryptic
function with eight parameters,

00:21:18.296 --> 00:21:21.346 A:middle
you call LA solve and
you give it the vector

00:21:21.446 --> 00:21:22.906 A:middle
and matrix you want
to solve for.

00:21:24.446 --> 00:21:25.626 A:middle
So that's very nice.

00:21:26.486 --> 00:21:31.546 A:middle
The past few years we have done
a little talk about LINPACK

00:21:32.506 --> 00:21:34.346 A:middle
and how fast we can
do linear algebra.

00:21:34.936 --> 00:21:38.486 A:middle
The LINPACK benchmark says how
fast can you solve a system

00:21:38.486 --> 00:21:39.406 A:middle
of linear equations?

00:21:39.446 --> 00:21:43.226 A:middle
It's exactly the operation
we saw in that last slide.

00:21:44.486 --> 00:21:48.096 A:middle
And historically, there have
been three different variations

00:21:48.096 --> 00:21:48.936 A:middle
on this benchmark.

00:21:49.406 --> 00:21:52.766 A:middle
It started out as how fast can
you solve a 100 by 100 system?

00:21:54.146 --> 00:21:56.836 A:middle
But as computers became
more powerful and faster,

00:21:57.256 --> 00:22:00.166 A:middle
it wasn't possible to actually
show how fast you were anymore



00:21:57.256 --> 00:22:00.166 A:middle
it wasn't possible to actually
show how fast you were anymore

00:22:00.306 --> 00:22:01.436 A:middle
with such a small problem.

00:22:01.436 --> 00:22:03.956 A:middle
So it became a 1,000
by 1,000 system.

00:22:05.326 --> 00:22:07.196 A:middle
And today when people
talk about LINPACK,

00:22:07.306 --> 00:22:09.426 A:middle
they actually mean
no holds barred.

00:22:09.426 --> 00:22:12.136 A:middle
You get to pick however
big a matrix you need

00:22:12.136 --> 00:22:13.196 A:middle
to show off how fast you are.

00:22:13.836 --> 00:22:16.446 A:middle
When you see supercomputer
rankings, they are talking

00:22:16.446 --> 00:22:18.326 A:middle
about ratings that are
millions by millions

00:22:18.326 --> 00:22:20.106 A:middle
when they give you
LINPACK scores.

00:22:20.106 --> 00:22:23.336 A:middle
You just pick whatever
makes you fastest.

00:22:23.606 --> 00:22:25.496 A:middle
So I was reading the
internet a few weeks ago

00:22:26.036 --> 00:22:27.236 A:middle
and saw someone talking

00:22:27.346 --> 00:22:30.426 A:middle
about now amazingly fast
their iPad Air 2 was.

00:22:30.806 --> 00:22:35.846 A:middle
It got 1.8 gigaflops on LINPACK,
which sounds pretty impressive.

00:22:36.756 --> 00:22:41.396 A:middle
It's 1.8 billion floating-point
operations per second.

00:22:41.396 --> 00:22:43.386 A:middle
I know that this number is low.

00:22:43.806 --> 00:22:49.296 A:middle
So someone had written a simple
C language routine to solve it

00:22:49.296 --> 00:22:51.146 A:middle
and just taken whatever
the compiler gave them

00:22:51.746 --> 00:22:53.886 A:middle
and that was what the
LINPACK score was.

00:22:54.426 --> 00:22:55.596 A:middle
So I looked around
a little bit more.

00:22:55.596 --> 00:22:59.436 A:middle
I found someone who had
carefully optimized their

00:22:59.436 --> 00:23:01.846 A:middle
LINPACK routines; they
had written vector code,



00:22:59.436 --> 00:23:01.846 A:middle
LINPACK routines; they
had written vector code,

00:23:01.996 --> 00:23:04.776 A:middle
they had cache tiled, they
had done multithreading,

00:23:04.886 --> 00:23:08.466 A:middle
and they got 5.6
gigaflops LINPACK.

00:23:08.996 --> 00:23:12.196 A:middle
It's three times
faster, nice improvement,

00:23:13.176 --> 00:23:15.696 A:middle
but rather than doing
all of that work,

00:23:16.676 --> 00:23:18.376 A:middle
you could just call
accelerate instead.

00:23:18.566 --> 00:23:19.986 A:middle
I showed you that D get RS

00:23:20.266 --> 00:23:22.426 A:middle
and LA solve function
a few slides ago.

00:23:22.926 --> 00:23:27.396 A:middle
You can write one line of code

00:23:29.826 --> 00:23:35.246 A:middle
and if you do that,
you get 25 gigaflops.

00:23:36.516 --> 00:23:41.546 A:middle
[ Applause ]

00:23:42.046 --> 00:23:43.926 A:middle
So this is what we
really want to do.

00:23:43.926 --> 00:23:46.086 A:middle
And this is what we want
to make easy for you guys.

00:23:46.086 --> 00:23:49.526 A:middle
We want to make it so you can
write one line of code rather

00:23:49.526 --> 00:23:51.326 A:middle
than having to hand
optimize everything,

00:23:52.116 --> 00:23:55.626 A:middle
and get great energy usage
and great performance

00:23:55.626 --> 00:23:57.046 A:middle
without needing to
do a lot of work.

00:23:58.036 --> 00:24:01.036 A:middle
So now I'm going to hand you
over to Luke, who will tell you



00:23:58.036 --> 00:24:01.036 A:middle
So now I'm going to hand you
over to Luke, who will tell you

00:24:01.036 --> 00:24:03.256 A:middle
about what you can do when
your matrixes are even

00:24:03.256 --> 00:24:03.546 A:middle
bigger [applause].

00:24:06.756 --> 00:24:09.356 A:middle
&gt;&gt; LUKE CHANG: Thank you,
Steve, my name's Luke Chang.

00:24:09.636 --> 00:24:12.206 A:middle
I'm an engineer on the
vector numerics group.

00:24:12.556 --> 00:24:15.106 A:middle
I'm here today to talk
about Sparse BLAS.

00:24:17.406 --> 00:24:20.796 A:middle
BLAS stands for basic linear
algebra solve programs.

00:24:21.556 --> 00:24:24.826 A:middle
And sparse BLAS is BLAS
for sparse matrices.

00:24:25.656 --> 00:24:29.746 A:middle
It's a new library
in iOS 9 and OS X.11.

00:24:31.016 --> 00:24:33.836 A:middle
It's designed with simple
API and good performance.

00:24:34.296 --> 00:24:36.466 A:middle
It supports both single
and double precision.

00:24:37.106 --> 00:24:40.476 A:middle
Why do we need sparse BLAS?

00:24:41.266 --> 00:24:44.756 A:middle
Can I use a dense BLAS
on the sparse matrix

00:24:45.126 --> 00:24:47.816 A:middle
that I already know
how it works.

00:24:47.816 --> 00:24:50.326 A:middle
Yes, it can given
that the dimension

00:24:50.326 --> 00:24:52.046 A:middle
of your matrix is not large,

00:24:53.086 --> 00:24:56.576 A:middle
but using Sparse BLAS is
better in so many ways.

00:24:57.626 --> 00:24:58.836 A:middle
In order to demonstrate that,

00:24:59.806 --> 00:25:02.446 A:middle
let me show you a
typical Sparse matrix.



00:24:59.806 --> 00:25:02.446 A:middle
let me show you a
typical Sparse matrix.

00:25:03.156 --> 00:25:05.606 A:middle
This is a typical matrix
from a machine learning.

00:25:06.036 --> 00:25:10.326 A:middle
It has more than a million
rows and 200,000 columns.

00:25:11.046 --> 00:25:14.646 A:middle
That gives us more than
300 billion entries.

00:25:15.566 --> 00:25:19.506 A:middle
However, the density of
the matrix is just 0.05%.

00:25:20.726 --> 00:25:23.186 A:middle
Here is a picture to
visualize the sparse matrix.

00:25:24.156 --> 00:25:26.316 A:middle
The darker the blue,
the higher the density.

00:25:27.096 --> 00:25:28.856 A:middle
As you can see there
is a lot of area

00:25:28.856 --> 00:25:29.916 A:middle
in this picture that is white.

00:25:30.536 --> 00:25:31.486 A:middle
They are just zeros.

00:25:31.986 --> 00:25:34.056 A:middle
We don't want to waste
memory on those zeros.

00:25:35.216 --> 00:25:37.986 A:middle
If you were to store
this sparse matrix

00:25:38.046 --> 00:25:39.596 A:middle
in a regular matrix format

00:25:40.166 --> 00:25:42.446 A:middle
so that you can use
the regular dense BLAS,

00:25:43.176 --> 00:25:47.476 A:middle
assuming just single precision
point numbers it takes more

00:25:47.476 --> 00:25:51.386 A:middle
than one terabyte of
memory, which is unfeasible.

00:25:51.526 --> 00:25:55.636 A:middle
You don't want to do that
on your phone, on your iPad.

00:25:57.106 --> 00:26:03.796 A:middle
So it saves memory, besides
sparse BLAS is also energy



00:25:57.106 --> 00:26:03.796 A:middle
So it saves memory, besides
sparse BLAS is also energy

00:26:03.796 --> 00:26:05.576 A:middle
efficient and faster.

00:26:06.176 --> 00:26:12.436 A:middle
We measure sparse BLAS
performance on a MacBook Pro,

00:26:12.436 --> 00:26:18.326 A:middle
13 inch at density 0.5%,
sparse BLAS is more

00:26:18.326 --> 00:26:22.176 A:middle
than 18 times more energy
efficient than dense BLAS.

00:26:23.396 --> 00:26:29.926 A:middle
Performance is 15 times faster.

00:26:29.926 --> 00:26:32.976 A:middle
As the density goes down,
performance goes up.

00:26:33.716 --> 00:26:39.476 A:middle
At 0.05%, which is the density
we saw in the previous matrix,

00:26:40.046 --> 00:26:43.026 A:middle
both energy efficiency
and performance are more

00:26:43.026 --> 00:26:47.116 A:middle
than 100 times better than
using a regular dense BLAS.

00:26:48.516 --> 00:26:50.476 A:middle
So now you see there are
many compelling reasons

00:26:50.476 --> 00:26:51.636 A:middle
to use sparse BLAS.

00:26:51.636 --> 00:26:54.686 A:middle
It saves memory, it's
energy efficient, faster.

00:26:55.516 --> 00:26:57.586 A:middle
What are available
in sparse BLAS?

00:26:58.876 --> 00:27:03.306 A:middle
We have products, triangular
solves, we can calculate norm,



00:26:58.876 --> 00:27:03.306 A:middle
We have products, triangular
solves, we can calculate norm,

00:27:03.696 --> 00:27:07.186 A:middle
L1, L2, L infinity
norms, et cetera.

00:27:07.996 --> 00:27:10.646 A:middle
We encounter trace, which
is the sum of diagonal.

00:27:11.376 --> 00:27:15.866 A:middle
It can permute rows and
columns, add new values a matrix

00:27:16.006 --> 00:27:18.416 A:middle
and extract rows and
columns from a matrix.

00:27:19.676 --> 00:27:22.396 A:middle
Before I talk about
the operations,

00:27:22.686 --> 00:27:25.356 A:middle
let's see how we
store a sparse vector.

00:27:26.606 --> 00:27:28.936 A:middle
Like I said before, we
don't want to waste memory

00:27:28.936 --> 00:27:31.666 A:middle
on the zero values, so we store

00:27:31.666 --> 00:27:34.006 A:middle
in the non-zero values
like this.

00:27:34.526 --> 00:27:39.346 A:middle
And then we need to know where
those non-zero values come from,

00:27:40.016 --> 00:27:42.586 A:middle
so we store the indices
of non-zero values.

00:27:43.636 --> 00:27:45.896 A:middle
Lastly we need to know
how many we stored

00:27:46.016 --> 00:27:48.036 A:middle
so that's the number
of non-zero values.

00:27:49.146 --> 00:27:52.316 A:middle
Those are the three things we
need to store a sparse vector.

00:27:52.916 --> 00:27:58.446 A:middle
In order to make the conversion
from regular dense vector

00:27:58.446 --> 00:28:02.396 A:middle
to a sparse vector, Sparse
BLAS provides utility functions



00:27:58.446 --> 00:28:02.396 A:middle
to a sparse vector, Sparse
BLAS provides utility functions

00:28:02.686 --> 00:28:03.906 A:middle
to help you with the conversion.

00:28:04.086 --> 00:28:07.436 A:middle
If you want to convert from
dense to sparse, you call pack.

00:28:08.156 --> 00:28:09.996 A:middle
The other way around
you call unpack.

00:28:10.786 --> 00:28:12.966 A:middle
If you just want to
know the non-zero count,

00:28:12.966 --> 00:28:16.116 A:middle
you can call get
vector non-zero count.

00:28:16.376 --> 00:28:18.506 A:middle
Now, we can look
at a sparse matrix.

00:28:18.506 --> 00:28:19.386 A:middle
How do we store it?

00:28:20.396 --> 00:28:24.286 A:middle
We can view the sparse matrix
as a collection of rows,

00:28:24.676 --> 00:28:26.426 A:middle
or a collection of columns.

00:28:26.726 --> 00:28:30.146 A:middle
So we have compressed sparse
row, compressed sparse columns,

00:28:30.976 --> 00:28:33.166 A:middle
or we just forget
about rows and columns.

00:28:33.166 --> 00:28:33.946 A:middle
Those are messy.

00:28:34.266 --> 00:28:37.586 A:middle
I want to see the matrix as
a list of non-zero values,

00:28:37.766 --> 00:28:40.976 A:middle
so for each non-zero value
we store the column index

00:28:40.976 --> 00:28:42.806 A:middle
and row index.

00:28:43.766 --> 00:28:47.726 A:middle
There are many other more
storage formats, and each one

00:28:47.726 --> 00:28:49.756 A:middle
of them has its own
pros and cons.

00:28:50.596 --> 00:28:52.776 A:middle
And there might be a
best storage format

00:28:52.776 --> 00:28:56.136 A:middle
for a particular
operation you want to do.

00:28:56.376 --> 00:28:58.306 A:middle
We decided to make
things easier for you.

00:28:59.516 --> 00:29:01.716 A:middle
We defined a sparse
matrix data type.



00:28:59.516 --> 00:29:01.716 A:middle
We defined a sparse
matrix data type.

00:29:02.386 --> 00:29:05.856 A:middle
It is opaque pointer, so
if you want to use it,

00:29:05.856 --> 00:29:08.566 A:middle
you have to first create it
using the create function,

00:29:09.146 --> 00:29:11.936 A:middle
and you can operate on
it; after you are done,

00:29:12.146 --> 00:29:13.586 A:middle
you have to destroy it.

00:29:14.206 --> 00:29:15.676 A:middle
The benefit of doing this is

00:29:15.676 --> 00:29:18.006 A:middle
that we manage the
memory for you.

00:29:18.346 --> 00:29:20.606 A:middle
You don't have to worry
about allocating new memory

00:29:20.606 --> 00:29:22.916 A:middle
when you are trying to add
new values to the matrix,

00:29:23.366 --> 00:29:27.046 A:middle
or resize the buffer, or
freeing the buffer at the end.

00:29:27.266 --> 00:29:28.116 A:middle
We will do that.

00:29:29.616 --> 00:29:33.046 A:middle
Even better, we will pick
the best storage format.

00:29:33.696 --> 00:29:36.966 A:middle
For example, if you
insert a bunch of rows

00:29:36.966 --> 00:29:40.206 A:middle
into the sparse matrix, we
will store it as sparse row.

00:29:40.746 --> 00:29:44.876 A:middle
And we will store it as sparse
columns when you insert a bunch

00:29:44.876 --> 00:29:49.256 A:middle
of columns, and if you want
to do a particular operation

00:29:49.256 --> 00:29:52.076 A:middle
and there is a best storage
format, we will convert it

00:29:52.186 --> 00:29:54.306 A:middle
to that format automatically.

00:29:54.916 --> 00:29:58.756 A:middle
So lastly, this enables us
to hide dimension details

00:29:58.756 --> 00:30:01.636 A:middle
in our API, that
makes our API cleaner.



00:29:58.756 --> 00:30:01.636 A:middle
in our API, that
makes our API cleaner.

00:30:02.706 --> 00:30:04.696 A:middle
When you call a sparse function,

00:30:04.696 --> 00:30:06.376 A:middle
you don't have passing
the dimension

00:30:06.376 --> 00:30:09.516 A:middle
of the sparse matrix every time.

00:30:09.776 --> 00:30:10.796 A:middle
So here is an example.

00:30:12.366 --> 00:30:14.386 A:middle
First, you create
a sparse matrix.

00:30:14.956 --> 00:30:18.986 A:middle
And then you can insert a
value to a sparse matrix.

00:30:19.566 --> 00:30:23.856 A:middle
Or you can insert sparse
vectors as rows or as columns.

00:30:25.126 --> 00:30:27.676 A:middle
Then you commit the changes
to the sparse matrix.

00:30:27.896 --> 00:30:29.836 A:middle
I will talk about
commit function

00:30:29.836 --> 00:30:31.306 A:middle
in the next slide
in more detail.

00:30:32.196 --> 00:30:34.036 A:middle
After you are done,
you call destroy.

00:30:35.306 --> 00:30:37.576 A:middle
Okay. Why do we need
a commit function?

00:30:38.356 --> 00:30:42.936 A:middle
Data insertion is expensive
because we store values

00:30:42.936 --> 00:30:44.156 A:middle
in a compressed format.

00:30:44.766 --> 00:30:47.616 A:middle
Every time you want to add a
value to a compressed storage

00:30:48.016 --> 00:30:49.866 A:middle
that involves data movement,

00:30:50.256 --> 00:30:53.756 A:middle
even more memory
allocation, that is expensive.

00:30:54.806 --> 00:30:56.986 A:middle
So we want to delay all
of the data insertion

00:30:56.986 --> 00:30:58.546 A:middle
so we can do them in batch.

00:30:59.106 --> 00:31:04.736 A:middle
Good news is, even you forget
to call the commit function,



00:30:59.106 --> 00:31:04.736 A:middle
Good news is, even you forget
to call the commit function,

00:31:05.216 --> 00:31:07.556 A:middle
the commit function will
be triggered automatically

00:31:08.026 --> 00:31:10.046 A:middle
when you do an operation
on the matrix.

00:31:10.716 --> 00:31:13.186 A:middle
Now, you may ask, then why
do I need a commit function?

00:31:13.456 --> 00:31:16.926 A:middle
If you want to control the
performance of your code,

00:31:17.566 --> 00:31:20.116 A:middle
you will want to use the
commit function explicitly.

00:31:20.666 --> 00:31:24.376 A:middle
Let's say you want to get your
sparse matrix ready during the

00:31:24.376 --> 00:31:27.586 A:middle
startup time of your app,
so your app can respond

00:31:27.586 --> 00:31:29.836 A:middle
to the user input as
quickly as possible.

00:31:30.716 --> 00:31:34.626 A:middle
Then you add the commit
function into your startup code.

00:31:35.106 --> 00:31:35.866 A:middle
That will do the trick.

00:31:36.856 --> 00:31:37.466 A:middle
All right.

00:31:37.896 --> 00:31:40.076 A:middle
So in the interest of
time, I will only talk

00:31:40.076 --> 00:31:42.916 A:middle
about two most common
operations in sparse BLAS.

00:31:42.916 --> 00:31:46.186 A:middle
The first is product,
C equals A times B.

00:31:46.846 --> 00:31:49.766 A:middle
We support vector inner
product, vector outer product,

00:31:50.306 --> 00:31:52.956 A:middle
matrix vector products and
matrix-matrix products.

00:31:54.026 --> 00:31:55.896 A:middle
What are the types for A, B, C?

00:31:56.236 --> 00:31:59.356 A:middle
For inner product, A is
sparse, B is either sparse

00:31:59.356 --> 00:32:01.566 A:middle
or dense, C is a single value.



00:31:59.356 --> 00:32:01.566 A:middle
or dense, C is a single value.

00:32:02.486 --> 00:32:05.556 A:middle
For outer product, A
is dense, B is sparse,

00:32:05.826 --> 00:32:08.176 A:middle
and the result C
is a sparse matrix.

00:32:08.966 --> 00:32:11.366 A:middle
For matrix vector and
matrix-matrix product,

00:32:11.496 --> 00:32:14.046 A:middle
A is sparse, B is
dense, C is dense.

00:32:15.126 --> 00:32:18.546 A:middle
It's very rare you need a
product of two sparse matrices

00:32:18.606 --> 00:32:19.576 A:middle
so it's not supported.

00:32:20.126 --> 00:32:24.766 A:middle
Here is the function
prototype of matrix product.

00:32:26.156 --> 00:32:28.136 A:middle
The naming convention
of sparse function is,

00:32:28.406 --> 00:32:30.526 A:middle
we start with sparse
underscore followed

00:32:30.526 --> 00:32:33.786 A:middle
by the operation we want to do,
in this case is matrix multiply.

00:32:34.896 --> 00:32:38.226 A:middle
Dense means B and C are
dense, float is the data type.

00:32:39.186 --> 00:32:42.156 A:middle
And you will return
success or an error code.

00:32:42.966 --> 00:32:45.946 A:middle
The argument of this function
is just like regular BLAS.

00:32:46.026 --> 00:32:49.546 A:middle
You specify the order of B and
C, column major or row major.

00:32:49.756 --> 00:32:51.996 A:middle
You say you want to
transpose on A or not,

00:32:52.786 --> 00:32:54.236 A:middle
number of columns of B and C.

00:32:54.426 --> 00:32:55.966 A:middle
The rest is just like BLAS.

00:32:56.936 --> 00:33:00.496 A:middle
The next operation
is triangular solve.



00:32:56.936 --> 00:33:00.496 A:middle
The next operation
is triangular solve.

00:33:01.146 --> 00:33:03.106 A:middle
You solve the triangular
system of equations,

00:33:03.756 --> 00:33:08.136 A:middle
so T has to be an upper or
lower triangular matrix.

00:33:09.186 --> 00:33:14.096 A:middle
We support both dense and,
dense vector or matrix for B,

00:33:15.296 --> 00:33:17.756 A:middle
one note to keep
in mind that upper

00:33:17.756 --> 00:33:21.296 A:middle
or lower triangular property
must be set before you do

00:33:21.296 --> 00:33:22.126 A:middle
data insertion.

00:33:23.036 --> 00:33:24.646 A:middle
Here is the code
to highlight that.

00:33:25.636 --> 00:33:28.446 A:middle
Set matrix property is
called before you do any

00:33:28.446 --> 00:33:29.296 A:middle
data insertion.

00:33:29.796 --> 00:33:32.336 A:middle
After data insertion, you
can call triangular solve,

00:33:32.826 --> 00:33:35.686 A:middle
and the argument,
again, is just like BLAS.

00:33:36.256 --> 00:33:42.316 A:middle
In summary, sparse BLAS we
design it with simple API,

00:33:42.616 --> 00:33:45.166 A:middle
it has a wide range
of operations,

00:33:45.636 --> 00:33:46.886 A:middle
and it has good performance.

00:33:48.196 --> 00:33:50.836 A:middle
Okay. Now it's time to
wrap up our session.

00:33:51.776 --> 00:33:53.936 A:middle
We talked about three
new libraries today,

00:33:54.646 --> 00:33:58.876 A:middle
compression with our own
new compressor LZFSE.

00:33:59.686 --> 00:34:01.456 A:middle
Now, you can use SIMD on Swift,



00:33:59.686 --> 00:34:01.456 A:middle
Now, you can use SIMD on Swift,

00:34:02.096 --> 00:34:04.156 A:middle
and there is a sparse
BLAS library.

00:34:05.446 --> 00:34:07.226 A:middle
They share the same
design goals.

00:34:07.836 --> 00:34:11.606 A:middle
They are fast, energy
efficient, and easy to use.

00:34:12.166 --> 00:34:14.016 A:middle
We encourage you
to give them a try

00:34:14.016 --> 00:34:15.466 A:middle
and let us know what you think.

00:34:16.476 --> 00:34:18.726 A:middle
We would love to hear from you.

00:34:18.946 --> 00:34:22.496 A:middle
We take developer requests
and feedback very seriously.

00:34:22.766 --> 00:34:25.456 A:middle
In fact, there are
many features we add

00:34:25.456 --> 00:34:27.335 A:middle
to accelerate framework based

00:34:27.335 --> 00:34:30.056 A:middle
on developer requests,
your requests.

00:34:30.466 --> 00:34:33.516 A:middle
So if you see something
missing that you want to use,

00:34:33.826 --> 00:34:35.206 A:middle
please file a feature request.

00:34:35.706 --> 00:34:39.636 A:middle
For more information, we
have online documentation

00:34:39.636 --> 00:34:41.485 A:middle
for vDSP and compression.

00:34:41.775 --> 00:34:45.536 A:middle
If you want to know more about
other parts of accelerate,

00:34:45.866 --> 00:34:49.196 A:middle
you can check out the video
of our previous WWDC sessions.

00:34:49.735 --> 00:34:53.326 A:middle
We also have simple code
for compression vDSP.

00:34:53.326 --> 00:34:56.246 A:middle
You can join the
discussion in the forum

00:34:56.656 --> 00:35:01.126 A:middle
or any general inquiries,
Paul Dembo is our evangelist.



00:34:56.656 --> 00:35:01.126 A:middle
or any general inquiries,
Paul Dembo is our evangelist.

00:35:02.546 --> 00:35:06.786 A:middle
Here are the related sessions
we talk about model I/0, Metal,

00:35:06.786 --> 00:35:11.126 A:middle
Swift, in our talk you can check
out the videos of these sessions

00:35:11.126 --> 00:35:12.016 A:middle
if you want to learn more.

00:35:12.476 --> 00:35:13.016 A:middle
That's it.

00:35:13.596 --> 00:35:14.476 A:middle
Thank you for coming.

00:35:14.476 --> 00:35:15.996 A:middle
We look forward to
seeing you in the Lab.

00:35:15.996 --> 00:35:16.876 A:middle
Thank you very much!

00:35:18.516 --> 00:35:32.680 A:middle
[ Applause ]

